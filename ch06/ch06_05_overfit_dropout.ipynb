{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T10:56:55.567170Z",
     "start_time": "2024-10-19T10:56:55.562918Z"
    }
   },
   "source": [
    "# %load overfit_dropout.py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\\\\deep_learning_from_scratch\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "def box_off():\n",
    "    ax=plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T10:57:53.970350Z",
     "start_time": "2024-10-19T10:57:05.372781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "# use_dropout = False  # 드롭아웃을 쓰지 않을 때는 False\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2 #드랍아웃 비율 설정\n",
    "# ====================================================\n",
    "\n",
    "# num_epochs = 301\n",
    "num_epochs = 501\n",
    "# num_epochs = 1001\n",
    "# num_epochs = 1501\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=num_epochs, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ],
   "id": "b35cc7fdb55f085c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3648801872969387\n",
      "=== epoch:1, train acc:0.09333333333333334, test acc:0.11 ===\n",
      "train loss:2.338296582065929\n",
      "train loss:2.3209117066505787\n",
      "train loss:2.322589909797648\n",
      "=== epoch:2, train acc:0.1, test acc:0.1093 ===\n",
      "train loss:2.3183786736585987\n",
      "train loss:2.3110743512337986\n",
      "train loss:2.319772920590909\n",
      "=== epoch:3, train acc:0.1, test acc:0.1098 ===\n",
      "train loss:2.3114445762001496\n",
      "train loss:2.314413307002033\n",
      "train loss:2.307632219984202\n",
      "=== epoch:4, train acc:0.1, test acc:0.1107 ===\n",
      "train loss:2.2989324156687885\n",
      "train loss:2.302002121077278\n",
      "train loss:2.3226242865903224\n",
      "=== epoch:5, train acc:0.10666666666666667, test acc:0.1109 ===\n",
      "train loss:2.3077655692204346\n",
      "train loss:2.2931695075168212\n",
      "train loss:2.3193791110943085\n",
      "=== epoch:6, train acc:0.11, test acc:0.1118 ===\n",
      "train loss:2.3328721552623084\n",
      "train loss:2.3042104240505745\n",
      "train loss:2.2983747717440983\n",
      "=== epoch:7, train acc:0.11, test acc:0.1132 ===\n",
      "train loss:2.2855994233051855\n",
      "train loss:2.315434451779298\n",
      "train loss:2.2966560827188705\n",
      "=== epoch:8, train acc:0.11333333333333333, test acc:0.114 ===\n",
      "train loss:2.316107138059032\n",
      "train loss:2.3077034055336707\n",
      "train loss:2.315260761742196\n",
      "=== epoch:9, train acc:0.11666666666666667, test acc:0.1146 ===\n",
      "train loss:2.3088849442675357\n",
      "train loss:2.295654737539037\n",
      "train loss:2.2951484919100964\n",
      "=== epoch:10, train acc:0.11, test acc:0.1154 ===\n",
      "train loss:2.2839315392985444\n",
      "train loss:2.3015542984035067\n",
      "train loss:2.301646899616618\n",
      "=== epoch:11, train acc:0.10666666666666667, test acc:0.1164 ===\n",
      "train loss:2.3015433886965426\n",
      "train loss:2.2870931430893346\n",
      "train loss:2.3055161214244895\n",
      "=== epoch:12, train acc:0.11, test acc:0.1205 ===\n",
      "train loss:2.3080290794156024\n",
      "train loss:2.288399477027749\n",
      "train loss:2.2867208595295496\n",
      "=== epoch:13, train acc:0.12, test acc:0.1248 ===\n",
      "train loss:2.305339053462186\n",
      "train loss:2.2815107400927244\n",
      "train loss:2.272882342719717\n",
      "=== epoch:14, train acc:0.12333333333333334, test acc:0.1271 ===\n",
      "train loss:2.292074947156698\n",
      "train loss:2.2800037822750086\n",
      "train loss:2.3075047153871187\n",
      "=== epoch:15, train acc:0.12, test acc:0.1311 ===\n",
      "train loss:2.283955402530005\n",
      "train loss:2.2841630584833195\n",
      "train loss:2.288448914345362\n",
      "=== epoch:16, train acc:0.13, test acc:0.1356 ===\n",
      "train loss:2.2862805779824056\n",
      "train loss:2.298562976651387\n",
      "train loss:2.2812313405150593\n",
      "=== epoch:17, train acc:0.14333333333333334, test acc:0.1396 ===\n",
      "train loss:2.271018839873758\n",
      "train loss:2.29299605984766\n",
      "train loss:2.283493957436896\n",
      "=== epoch:18, train acc:0.13666666666666666, test acc:0.1432 ===\n",
      "train loss:2.295461661801522\n",
      "train loss:2.2798359296246704\n",
      "train loss:2.3070116022160745\n",
      "=== epoch:19, train acc:0.15, test acc:0.1472 ===\n",
      "train loss:2.27992829639388\n",
      "train loss:2.2768711803427104\n",
      "train loss:2.287881073348801\n",
      "=== epoch:20, train acc:0.16666666666666666, test acc:0.1512 ===\n",
      "train loss:2.2830071163699954\n",
      "train loss:2.2743906454264153\n",
      "train loss:2.287860251606783\n",
      "=== epoch:21, train acc:0.16666666666666666, test acc:0.154 ===\n",
      "train loss:2.2774879294498094\n",
      "train loss:2.287413661891912\n",
      "train loss:2.291511495642828\n",
      "=== epoch:22, train acc:0.17, test acc:0.1587 ===\n",
      "train loss:2.2800492000839805\n",
      "train loss:2.2870497714357483\n",
      "train loss:2.292131668860695\n",
      "=== epoch:23, train acc:0.18666666666666668, test acc:0.1656 ===\n",
      "train loss:2.2863846342726144\n",
      "train loss:2.2747676161213266\n",
      "train loss:2.281409863779431\n",
      "=== epoch:24, train acc:0.18333333333333332, test acc:0.1729 ===\n",
      "train loss:2.279925544293457\n",
      "train loss:2.2886176184663274\n",
      "train loss:2.29832550522463\n",
      "=== epoch:25, train acc:0.18, test acc:0.172 ===\n",
      "train loss:2.2746175418553407\n",
      "train loss:2.2847888926530038\n",
      "train loss:2.2709269922361575\n",
      "=== epoch:26, train acc:0.19333333333333333, test acc:0.1722 ===\n",
      "train loss:2.2724351928629445\n",
      "train loss:2.278566190663129\n",
      "train loss:2.2767641372604923\n",
      "=== epoch:27, train acc:0.20666666666666667, test acc:0.1734 ===\n",
      "train loss:2.2837989636715177\n",
      "train loss:2.2574888800836876\n",
      "train loss:2.277981216276346\n",
      "=== epoch:28, train acc:0.20666666666666667, test acc:0.1768 ===\n",
      "train loss:2.279066910198272\n",
      "train loss:2.269375068604378\n",
      "train loss:2.277668319471936\n",
      "=== epoch:29, train acc:0.20666666666666667, test acc:0.1786 ===\n",
      "train loss:2.2827186915857616\n",
      "train loss:2.2656641895580067\n",
      "train loss:2.275098841303538\n",
      "=== epoch:30, train acc:0.20666666666666667, test acc:0.1778 ===\n",
      "train loss:2.2674777227920524\n",
      "train loss:2.278898395559483\n",
      "train loss:2.2641009528731546\n",
      "=== epoch:31, train acc:0.21666666666666667, test acc:0.1823 ===\n",
      "train loss:2.2615006228108534\n",
      "train loss:2.269649956326032\n",
      "train loss:2.2815562896971144\n",
      "=== epoch:32, train acc:0.22, test acc:0.183 ===\n",
      "train loss:2.280743941482309\n",
      "train loss:2.285162457633392\n",
      "train loss:2.272990810860006\n",
      "=== epoch:33, train acc:0.21333333333333335, test acc:0.1795 ===\n",
      "train loss:2.2581330766435452\n",
      "train loss:2.2788312901118544\n",
      "train loss:2.278442306147773\n",
      "=== epoch:34, train acc:0.21333333333333335, test acc:0.1815 ===\n",
      "train loss:2.272830694017212\n",
      "train loss:2.271939673050467\n",
      "train loss:2.269109744664366\n",
      "=== epoch:35, train acc:0.21666666666666667, test acc:0.1783 ===\n",
      "train loss:2.270977626951797\n",
      "train loss:2.2632608395498504\n",
      "train loss:2.2721331051113625\n",
      "=== epoch:36, train acc:0.22, test acc:0.1778 ===\n",
      "train loss:2.266159400753515\n",
      "train loss:2.2765765220240692\n",
      "train loss:2.269336206148999\n",
      "=== epoch:37, train acc:0.21666666666666667, test acc:0.1827 ===\n",
      "train loss:2.2649708501272494\n",
      "train loss:2.262551522442983\n",
      "train loss:2.268389145475063\n",
      "=== epoch:38, train acc:0.22, test acc:0.1835 ===\n",
      "train loss:2.2719431137327244\n",
      "train loss:2.262738049698477\n",
      "train loss:2.25151943959287\n",
      "=== epoch:39, train acc:0.22666666666666666, test acc:0.185 ===\n",
      "train loss:2.270030272328419\n",
      "train loss:2.2635232271520613\n",
      "train loss:2.2711154788929706\n",
      "=== epoch:40, train acc:0.22333333333333333, test acc:0.1936 ===\n",
      "train loss:2.258293883342056\n",
      "train loss:2.269941151643011\n",
      "train loss:2.267735482701825\n",
      "=== epoch:41, train acc:0.24666666666666667, test acc:0.2023 ===\n",
      "train loss:2.269941115077066\n",
      "train loss:2.2640629290255565\n",
      "train loss:2.257543761914671\n",
      "=== epoch:42, train acc:0.23666666666666666, test acc:0.2042 ===\n",
      "train loss:2.237149456749048\n",
      "train loss:2.255104248389864\n",
      "train loss:2.2591310658648776\n",
      "=== epoch:43, train acc:0.24, test acc:0.2046 ===\n",
      "train loss:2.2568649121258666\n",
      "train loss:2.2574301367634395\n",
      "train loss:2.2744608479569486\n",
      "=== epoch:44, train acc:0.24666666666666667, test acc:0.2057 ===\n",
      "train loss:2.2513612188034116\n",
      "train loss:2.270777315263477\n",
      "train loss:2.263238194696852\n",
      "=== epoch:45, train acc:0.25666666666666665, test acc:0.2098 ===\n",
      "train loss:2.257676230729177\n",
      "train loss:2.266141891722179\n",
      "train loss:2.262863575878841\n",
      "=== epoch:46, train acc:0.26, test acc:0.2179 ===\n",
      "train loss:2.2528587159158304\n",
      "train loss:2.2509644945721736\n",
      "train loss:2.25151585612932\n",
      "=== epoch:47, train acc:0.2633333333333333, test acc:0.2224 ===\n",
      "train loss:2.2540547733016516\n",
      "train loss:2.2406345669772367\n",
      "train loss:2.240600826360131\n",
      "=== epoch:48, train acc:0.28, test acc:0.2235 ===\n",
      "train loss:2.261028309559736\n",
      "train loss:2.2519336289512437\n",
      "train loss:2.241221609763816\n",
      "=== epoch:49, train acc:0.2833333333333333, test acc:0.2265 ===\n",
      "train loss:2.255801462900578\n",
      "train loss:2.2547230357170527\n",
      "train loss:2.258133716979488\n",
      "=== epoch:50, train acc:0.29, test acc:0.2308 ===\n",
      "train loss:2.2495180028585184\n",
      "train loss:2.246419983015699\n",
      "train loss:2.2521334259395624\n",
      "=== epoch:51, train acc:0.29, test acc:0.2332 ===\n",
      "train loss:2.2548993016953536\n",
      "train loss:2.257390786174189\n",
      "train loss:2.2547356984246734\n",
      "=== epoch:52, train acc:0.32, test acc:0.2418 ===\n",
      "train loss:2.240458640664986\n",
      "train loss:2.237314662839286\n",
      "train loss:2.2456082244375257\n",
      "=== epoch:53, train acc:0.31333333333333335, test acc:0.2403 ===\n",
      "train loss:2.2399122262191087\n",
      "train loss:2.2327709273729455\n",
      "train loss:2.266419072254669\n",
      "=== epoch:54, train acc:0.32, test acc:0.2514 ===\n",
      "train loss:2.2593662962191505\n",
      "train loss:2.243899418421333\n",
      "train loss:2.2383757223274436\n",
      "=== epoch:55, train acc:0.32666666666666666, test acc:0.2529 ===\n",
      "train loss:2.2377277642427433\n",
      "train loss:2.2509453231238976\n",
      "train loss:2.252716402588051\n",
      "=== epoch:56, train acc:0.33, test acc:0.2526 ===\n",
      "train loss:2.2397824361335106\n",
      "train loss:2.2419368250354466\n",
      "train loss:2.2527190126088956\n",
      "=== epoch:57, train acc:0.33, test acc:0.2598 ===\n",
      "train loss:2.21432619443264\n",
      "train loss:2.233971225187779\n",
      "train loss:2.233328401537515\n",
      "=== epoch:58, train acc:0.33, test acc:0.2595 ===\n",
      "train loss:2.236354287312612\n",
      "train loss:2.236078041199356\n",
      "train loss:2.2435865546444234\n",
      "=== epoch:59, train acc:0.33666666666666667, test acc:0.2659 ===\n",
      "train loss:2.242397248976569\n",
      "train loss:2.2308573646006034\n",
      "train loss:2.2279076093540686\n",
      "=== epoch:60, train acc:0.33666666666666667, test acc:0.2718 ===\n",
      "train loss:2.219213204636438\n",
      "train loss:2.233028946922336\n",
      "train loss:2.2272700052267247\n",
      "=== epoch:61, train acc:0.34, test acc:0.2708 ===\n",
      "train loss:2.220714539037104\n",
      "train loss:2.2227379303742323\n",
      "train loss:2.2283389662055906\n",
      "=== epoch:62, train acc:0.33666666666666667, test acc:0.274 ===\n",
      "train loss:2.229388280500866\n",
      "train loss:2.2337578012166794\n",
      "train loss:2.2417963191645756\n",
      "=== epoch:63, train acc:0.3466666666666667, test acc:0.2782 ===\n",
      "train loss:2.2277553885939367\n",
      "train loss:2.2446929039614516\n",
      "train loss:2.2321207464037154\n",
      "=== epoch:64, train acc:0.34, test acc:0.2796 ===\n",
      "train loss:2.2189565765325114\n",
      "train loss:2.2321050233340265\n",
      "train loss:2.2316271428122714\n",
      "=== epoch:65, train acc:0.3466666666666667, test acc:0.2793 ===\n",
      "train loss:2.256613491143481\n",
      "train loss:2.2315185338314296\n",
      "train loss:2.2243394475938563\n",
      "=== epoch:66, train acc:0.35, test acc:0.2828 ===\n",
      "train loss:2.2238595358010294\n",
      "train loss:2.2324987062263024\n",
      "train loss:2.2294312850687885\n",
      "=== epoch:67, train acc:0.3466666666666667, test acc:0.283 ===\n",
      "train loss:2.2201358722465194\n",
      "train loss:2.2188485226309265\n",
      "train loss:2.2178993337476394\n",
      "=== epoch:68, train acc:0.3466666666666667, test acc:0.2804 ===\n",
      "train loss:2.214290533843531\n",
      "train loss:2.209943492277755\n",
      "train loss:2.2180824255583023\n",
      "=== epoch:69, train acc:0.3433333333333333, test acc:0.2795 ===\n",
      "train loss:2.229676850664149\n",
      "train loss:2.2410376708452686\n",
      "train loss:2.2131999493188443\n",
      "=== epoch:70, train acc:0.35, test acc:0.2832 ===\n",
      "train loss:2.2310650838866675\n",
      "train loss:2.206294508976104\n",
      "train loss:2.199369844441805\n",
      "=== epoch:71, train acc:0.3566666666666667, test acc:0.2841 ===\n",
      "train loss:2.2004102014282174\n",
      "train loss:2.2245571330471225\n",
      "train loss:2.215900063524897\n",
      "=== epoch:72, train acc:0.35, test acc:0.2845 ===\n",
      "train loss:2.2141957268939247\n",
      "train loss:2.2231570037037742\n",
      "train loss:2.2220310882359975\n",
      "=== epoch:73, train acc:0.35333333333333333, test acc:0.2864 ===\n",
      "train loss:2.1976423834683656\n",
      "train loss:2.2182295361327693\n",
      "train loss:2.211524413886069\n",
      "=== epoch:74, train acc:0.35333333333333333, test acc:0.2891 ===\n",
      "train loss:2.218271855545703\n",
      "train loss:2.2204240621883318\n",
      "train loss:2.2049130961786596\n",
      "=== epoch:75, train acc:0.35333333333333333, test acc:0.2888 ===\n",
      "train loss:2.2148192617884326\n",
      "train loss:2.2132534323789406\n",
      "train loss:2.2111972993449927\n",
      "=== epoch:76, train acc:0.36333333333333334, test acc:0.2961 ===\n",
      "train loss:2.205501764311179\n",
      "train loss:2.216745318609529\n",
      "train loss:2.200428759553383\n",
      "=== epoch:77, train acc:0.35333333333333333, test acc:0.2985 ===\n",
      "train loss:2.205782329610555\n",
      "train loss:2.201838063442914\n",
      "train loss:2.1955436827486845\n",
      "=== epoch:78, train acc:0.3566666666666667, test acc:0.2974 ===\n",
      "train loss:2.2179188812462156\n",
      "train loss:2.232845028033855\n",
      "train loss:2.2007273345905687\n",
      "=== epoch:79, train acc:0.35, test acc:0.2982 ===\n",
      "train loss:2.2047361545404875\n",
      "train loss:2.1852441161470546\n",
      "train loss:2.204825844755711\n",
      "=== epoch:80, train acc:0.35333333333333333, test acc:0.2992 ===\n",
      "train loss:2.2143098758301605\n",
      "train loss:2.1875679079149557\n",
      "train loss:2.1839821783047815\n",
      "=== epoch:81, train acc:0.36, test acc:0.3015 ===\n",
      "train loss:2.2288789695127122\n",
      "train loss:2.2344970930789194\n",
      "train loss:2.207852881621552\n",
      "=== epoch:82, train acc:0.36, test acc:0.3033 ===\n",
      "train loss:2.22206904572656\n",
      "train loss:2.206131197072782\n",
      "train loss:2.1962485351787313\n",
      "=== epoch:83, train acc:0.35333333333333333, test acc:0.3024 ===\n",
      "train loss:2.1991982094387668\n",
      "train loss:2.1920001382266463\n",
      "train loss:2.1834480527666598\n",
      "=== epoch:84, train acc:0.3566666666666667, test acc:0.309 ===\n",
      "train loss:2.1699390911254106\n",
      "train loss:2.2061643877896002\n",
      "train loss:2.1939221268339795\n",
      "=== epoch:85, train acc:0.36333333333333334, test acc:0.316 ===\n",
      "train loss:2.1751697855969927\n",
      "train loss:2.203939229346938\n",
      "train loss:2.189667618587278\n",
      "=== epoch:86, train acc:0.36, test acc:0.3127 ===\n",
      "train loss:2.198840670259006\n",
      "train loss:2.191514508785821\n",
      "train loss:2.1954704305606594\n",
      "=== epoch:87, train acc:0.36, test acc:0.3123 ===\n",
      "train loss:2.187329935935275\n",
      "train loss:2.1935444307014547\n",
      "train loss:2.2000503424935403\n",
      "=== epoch:88, train acc:0.3566666666666667, test acc:0.3158 ===\n",
      "train loss:2.185245005209\n",
      "train loss:2.206273425993569\n",
      "train loss:2.20425920961805\n",
      "=== epoch:89, train acc:0.37, test acc:0.318 ===\n",
      "train loss:2.217635712698377\n",
      "train loss:2.1732010820443977\n",
      "train loss:2.178990523007148\n",
      "=== epoch:90, train acc:0.37333333333333335, test acc:0.3199 ===\n",
      "train loss:2.193689102389408\n",
      "train loss:2.2143159067512173\n",
      "train loss:2.191527322484091\n",
      "=== epoch:91, train acc:0.37666666666666665, test acc:0.3214 ===\n",
      "train loss:2.2016898346387297\n",
      "train loss:2.1929856248364157\n",
      "train loss:2.197259635746795\n",
      "=== epoch:92, train acc:0.38, test acc:0.3206 ===\n",
      "train loss:2.196217945128168\n",
      "train loss:2.179053782252856\n",
      "train loss:2.166944081896961\n",
      "=== epoch:93, train acc:0.37666666666666665, test acc:0.3209 ===\n",
      "train loss:2.152143342845488\n",
      "train loss:2.1985082870269306\n",
      "train loss:2.1506961515373146\n",
      "=== epoch:94, train acc:0.37666666666666665, test acc:0.3159 ===\n",
      "train loss:2.160251024868338\n",
      "train loss:2.192927201409867\n",
      "train loss:2.2068233228260543\n",
      "=== epoch:95, train acc:0.37666666666666665, test acc:0.3236 ===\n",
      "train loss:2.177142728099601\n",
      "train loss:2.1909832526601\n",
      "train loss:2.1839774481045677\n",
      "=== epoch:96, train acc:0.38, test acc:0.3255 ===\n",
      "train loss:2.153454176185932\n",
      "train loss:2.1461686878292663\n",
      "train loss:2.1604207720551014\n",
      "=== epoch:97, train acc:0.37666666666666665, test acc:0.3222 ===\n",
      "train loss:2.140469028851827\n",
      "train loss:2.1599298626634553\n",
      "train loss:2.1509710208037016\n",
      "=== epoch:98, train acc:0.38666666666666666, test acc:0.3208 ===\n",
      "train loss:2.169018583136501\n",
      "train loss:2.1094543106530512\n",
      "train loss:2.156986823746233\n",
      "=== epoch:99, train acc:0.38666666666666666, test acc:0.3246 ===\n",
      "train loss:2.1833482182203006\n",
      "train loss:2.194549393319343\n",
      "train loss:2.153579194034052\n",
      "=== epoch:100, train acc:0.39666666666666667, test acc:0.3282 ===\n",
      "train loss:2.156611272051073\n",
      "train loss:2.1609599838808164\n",
      "train loss:2.1658397991025327\n",
      "=== epoch:101, train acc:0.4066666666666667, test acc:0.3353 ===\n",
      "train loss:2.185331762641997\n",
      "train loss:2.140106220694087\n",
      "train loss:2.1691170698934688\n",
      "=== epoch:102, train acc:0.4166666666666667, test acc:0.3392 ===\n",
      "train loss:2.1739313103976325\n",
      "train loss:2.139553680296676\n",
      "train loss:2.1205901636254625\n",
      "=== epoch:103, train acc:0.41, test acc:0.3388 ===\n",
      "train loss:2.1456261540301758\n",
      "train loss:2.1622529520359457\n",
      "train loss:2.1597131922296766\n",
      "=== epoch:104, train acc:0.4166666666666667, test acc:0.3379 ===\n",
      "train loss:2.1705163127661735\n",
      "train loss:2.161633255102409\n",
      "train loss:2.1426877267383304\n",
      "=== epoch:105, train acc:0.41333333333333333, test acc:0.3419 ===\n",
      "train loss:2.1379112616694234\n",
      "train loss:2.142771023784115\n",
      "train loss:2.125751214410814\n",
      "=== epoch:106, train acc:0.41, test acc:0.3407 ===\n",
      "train loss:2.119791580282519\n",
      "train loss:2.1346793450345873\n",
      "train loss:2.1419654925320835\n",
      "=== epoch:107, train acc:0.4066666666666667, test acc:0.3383 ===\n",
      "train loss:2.1543917704454687\n",
      "train loss:2.1220307337670374\n",
      "train loss:2.142506772392955\n",
      "=== epoch:108, train acc:0.4066666666666667, test acc:0.3401 ===\n",
      "train loss:2.136065902878447\n",
      "train loss:2.1427124277029894\n",
      "train loss:2.145936066665367\n",
      "=== epoch:109, train acc:0.42333333333333334, test acc:0.3457 ===\n",
      "train loss:2.1262461662638725\n",
      "train loss:2.1386648128063652\n",
      "train loss:2.138047475970619\n",
      "=== epoch:110, train acc:0.42333333333333334, test acc:0.3459 ===\n",
      "train loss:2.146214891215519\n",
      "train loss:2.1205750828359258\n",
      "train loss:2.1411067008721854\n",
      "=== epoch:111, train acc:0.42333333333333334, test acc:0.3484 ===\n",
      "train loss:2.1280006585433915\n",
      "train loss:2.100766378052661\n",
      "train loss:2.112160386354134\n",
      "=== epoch:112, train acc:0.42, test acc:0.3482 ===\n",
      "train loss:2.1403889004411703\n",
      "train loss:2.0762408162932475\n",
      "train loss:2.1327551827130837\n",
      "=== epoch:113, train acc:0.4166666666666667, test acc:0.3449 ===\n",
      "train loss:2.1142389878423624\n",
      "train loss:2.146132798226394\n",
      "train loss:2.0924642523030963\n",
      "=== epoch:114, train acc:0.42, test acc:0.3501 ===\n",
      "train loss:2.105656417286736\n",
      "train loss:2.1389852353008405\n",
      "train loss:2.112963455588909\n",
      "=== epoch:115, train acc:0.41, test acc:0.3486 ===\n",
      "train loss:2.157104757006197\n",
      "train loss:2.0427022899646086\n",
      "train loss:2.1140795239397256\n",
      "=== epoch:116, train acc:0.41, test acc:0.3506 ===\n",
      "train loss:2.1063016525535136\n",
      "train loss:2.128226126850453\n",
      "train loss:2.070913867007233\n",
      "=== epoch:117, train acc:0.41333333333333333, test acc:0.3474 ===\n",
      "train loss:2.071341499966017\n",
      "train loss:2.0958697566189026\n",
      "train loss:2.081268428515775\n",
      "=== epoch:118, train acc:0.4066666666666667, test acc:0.3486 ===\n",
      "train loss:2.145226747339073\n",
      "train loss:2.1179094767249276\n",
      "train loss:2.087071855327015\n",
      "=== epoch:119, train acc:0.4166666666666667, test acc:0.352 ===\n",
      "train loss:2.120755705706398\n",
      "train loss:2.0836532713930027\n",
      "train loss:2.0954255495501113\n",
      "=== epoch:120, train acc:0.4166666666666667, test acc:0.3536 ===\n",
      "train loss:2.0953126327253706\n",
      "train loss:2.0900236103565897\n",
      "train loss:2.0565727343683857\n",
      "=== epoch:121, train acc:0.4266666666666667, test acc:0.3556 ===\n",
      "train loss:2.1027716880781924\n",
      "train loss:2.088546271651933\n",
      "train loss:2.096361017445295\n",
      "=== epoch:122, train acc:0.42333333333333334, test acc:0.3582 ===\n",
      "train loss:2.0649620366958406\n",
      "train loss:2.095221021140119\n",
      "train loss:2.0797314486901373\n",
      "=== epoch:123, train acc:0.43666666666666665, test acc:0.3653 ===\n",
      "train loss:2.0258502284213837\n",
      "train loss:2.069722790098312\n",
      "train loss:2.1120149422752688\n",
      "=== epoch:124, train acc:0.45, test acc:0.3677 ===\n",
      "train loss:2.0348970641446957\n",
      "train loss:2.0685024873374775\n",
      "train loss:2.0841358714954303\n",
      "=== epoch:125, train acc:0.44333333333333336, test acc:0.3687 ===\n",
      "train loss:2.035957972007758\n",
      "train loss:2.047773219067576\n",
      "train loss:2.0288371816463537\n",
      "=== epoch:126, train acc:0.44, test acc:0.3632 ===\n",
      "train loss:2.085118681219363\n",
      "train loss:2.0573780356531572\n",
      "train loss:2.0685913591813594\n",
      "=== epoch:127, train acc:0.45, test acc:0.3694 ===\n",
      "train loss:2.0470576969081016\n",
      "train loss:2.0609482901780147\n",
      "train loss:2.014473908665234\n",
      "=== epoch:128, train acc:0.44666666666666666, test acc:0.3712 ===\n",
      "train loss:2.0379438990973466\n",
      "train loss:2.0411266937658348\n",
      "train loss:2.036289871581737\n",
      "=== epoch:129, train acc:0.44666666666666666, test acc:0.3685 ===\n",
      "train loss:2.066886284701594\n",
      "train loss:2.0176709948085247\n",
      "train loss:2.069356749855584\n",
      "=== epoch:130, train acc:0.45, test acc:0.3715 ===\n",
      "train loss:2.041466119790717\n",
      "train loss:2.067460472795079\n",
      "train loss:2.004766894272065\n",
      "=== epoch:131, train acc:0.45, test acc:0.3729 ===\n",
      "train loss:2.0530457779486992\n",
      "train loss:2.025557477252629\n",
      "train loss:2.0485357844240233\n",
      "=== epoch:132, train acc:0.44666666666666666, test acc:0.3726 ===\n",
      "train loss:2.020270357104966\n",
      "train loss:2.0193823657878034\n",
      "train loss:2.025235904192787\n",
      "=== epoch:133, train acc:0.44666666666666666, test acc:0.3704 ===\n",
      "train loss:1.9926439200720305\n",
      "train loss:2.083190881325222\n",
      "train loss:2.010890876088425\n",
      "=== epoch:134, train acc:0.44333333333333336, test acc:0.3687 ===\n",
      "train loss:1.982118600567029\n",
      "train loss:1.961493954037151\n",
      "train loss:2.063508080616291\n",
      "=== epoch:135, train acc:0.45666666666666667, test acc:0.3689 ===\n",
      "train loss:2.0250931142178885\n",
      "train loss:1.9731849934239916\n",
      "train loss:2.0902066864390223\n",
      "=== epoch:136, train acc:0.45666666666666667, test acc:0.372 ===\n",
      "train loss:2.053843965090756\n",
      "train loss:1.964824003057573\n",
      "train loss:2.049696534318225\n",
      "=== epoch:137, train acc:0.47, test acc:0.3761 ===\n",
      "train loss:2.0668466592933434\n",
      "train loss:2.0001315489135454\n",
      "train loss:2.0179662882662175\n",
      "=== epoch:138, train acc:0.47333333333333333, test acc:0.3804 ===\n",
      "train loss:1.9746899036793326\n",
      "train loss:2.0431208545173543\n",
      "train loss:1.9890744242637737\n",
      "=== epoch:139, train acc:0.4866666666666667, test acc:0.3815 ===\n",
      "train loss:2.0356732630284147\n",
      "train loss:2.0931021489135895\n",
      "train loss:2.0543574311888313\n",
      "=== epoch:140, train acc:0.49666666666666665, test acc:0.3887 ===\n",
      "train loss:1.9757342828522595\n",
      "train loss:2.028071642199699\n",
      "train loss:1.986639823032728\n",
      "=== epoch:141, train acc:0.49333333333333335, test acc:0.3887 ===\n",
      "train loss:2.054989397498205\n",
      "train loss:2.0065510872442176\n",
      "train loss:1.9608752802571017\n",
      "=== epoch:142, train acc:0.5133333333333333, test acc:0.393 ===\n",
      "train loss:1.9509694264427961\n",
      "train loss:2.0029665625693256\n",
      "train loss:2.0301032945220636\n",
      "=== epoch:143, train acc:0.5066666666666667, test acc:0.394 ===\n",
      "train loss:1.9006553947205902\n",
      "train loss:1.9877565810366413\n",
      "train loss:2.0151284021768685\n",
      "=== epoch:144, train acc:0.5066666666666667, test acc:0.3936 ===\n",
      "train loss:2.017330564447735\n",
      "train loss:1.9962931544495712\n",
      "train loss:1.9532288649279\n",
      "=== epoch:145, train acc:0.51, test acc:0.3996 ===\n",
      "train loss:1.9796788745736813\n",
      "train loss:1.9433688053194873\n",
      "train loss:1.9871109213969558\n",
      "=== epoch:146, train acc:0.51, test acc:0.3997 ===\n",
      "train loss:2.003483065437806\n",
      "train loss:1.9370325565721485\n",
      "train loss:2.0446211576605133\n",
      "=== epoch:147, train acc:0.5133333333333333, test acc:0.4007 ===\n",
      "train loss:1.9332966849277178\n",
      "train loss:1.9833876504418477\n",
      "train loss:1.9576957021762014\n",
      "=== epoch:148, train acc:0.52, test acc:0.4034 ===\n",
      "train loss:1.9713664880533484\n",
      "train loss:2.0059834375877856\n",
      "train loss:1.996891678329105\n",
      "=== epoch:149, train acc:0.5333333333333333, test acc:0.4074 ===\n",
      "train loss:1.9154649917450317\n",
      "train loss:1.9494988112531106\n",
      "train loss:1.968455685014776\n",
      "=== epoch:150, train acc:0.53, test acc:0.4084 ===\n",
      "train loss:2.005054302576136\n",
      "train loss:1.9564706618353156\n",
      "train loss:2.0169570736113873\n",
      "=== epoch:151, train acc:0.5333333333333333, test acc:0.413 ===\n",
      "train loss:1.9245728151537822\n",
      "train loss:1.9358539281238847\n",
      "train loss:2.0179035755973755\n",
      "=== epoch:152, train acc:0.54, test acc:0.4202 ===\n",
      "train loss:2.0312283924150747\n",
      "train loss:1.873775702878652\n",
      "train loss:1.9214274973451753\n",
      "=== epoch:153, train acc:0.55, test acc:0.4228 ===\n",
      "train loss:1.9369912324085141\n",
      "train loss:1.8955788234219784\n",
      "train loss:1.966175278718821\n",
      "=== epoch:154, train acc:0.5433333333333333, test acc:0.4212 ===\n",
      "train loss:2.016119565231682\n",
      "train loss:1.9234888196957731\n",
      "train loss:1.9270463486629277\n",
      "=== epoch:155, train acc:0.55, test acc:0.4278 ===\n",
      "train loss:1.9397782780615893\n",
      "train loss:1.850665842588897\n",
      "train loss:1.883138606824887\n",
      "=== epoch:156, train acc:0.5566666666666666, test acc:0.4287 ===\n",
      "train loss:1.9596337899956617\n",
      "train loss:1.8668352707935643\n",
      "train loss:1.9669932447258598\n",
      "=== epoch:157, train acc:0.5566666666666666, test acc:0.4292 ===\n",
      "train loss:1.9208724149136427\n",
      "train loss:1.9344642602537332\n",
      "train loss:1.9303142936359878\n",
      "=== epoch:158, train acc:0.5466666666666666, test acc:0.4264 ===\n",
      "train loss:1.9206563953365994\n",
      "train loss:1.8582875942674104\n",
      "train loss:1.9754767264637794\n",
      "=== epoch:159, train acc:0.55, test acc:0.4258 ===\n",
      "train loss:1.9518730208847033\n",
      "train loss:1.8351368216062536\n",
      "train loss:1.9185230485282283\n",
      "=== epoch:160, train acc:0.5566666666666666, test acc:0.429 ===\n",
      "train loss:1.9727153326037807\n",
      "train loss:1.9355366423667852\n",
      "train loss:1.8571900206791119\n",
      "=== epoch:161, train acc:0.5633333333333334, test acc:0.4334 ===\n",
      "train loss:2.007831710924857\n",
      "train loss:1.9372420356750673\n",
      "train loss:1.9259245815755077\n",
      "=== epoch:162, train acc:0.5666666666666667, test acc:0.4378 ===\n",
      "train loss:1.8972387790421812\n",
      "train loss:1.8850464156996936\n",
      "train loss:1.8459546740599924\n",
      "=== epoch:163, train acc:0.57, test acc:0.4394 ===\n",
      "train loss:1.9286767806492362\n",
      "train loss:1.8893035578113997\n",
      "train loss:1.8880995536921477\n",
      "=== epoch:164, train acc:0.5733333333333334, test acc:0.4423 ===\n",
      "train loss:1.9448174896921941\n",
      "train loss:1.7842373795455606\n",
      "train loss:1.8829167159980287\n",
      "=== epoch:165, train acc:0.57, test acc:0.4382 ===\n",
      "train loss:1.948003180334247\n",
      "train loss:1.9252178579035453\n",
      "train loss:1.8817478045126723\n",
      "=== epoch:166, train acc:0.5766666666666667, test acc:0.4401 ===\n",
      "train loss:1.841933826773416\n",
      "train loss:1.8550849519906196\n",
      "train loss:1.8986824220410907\n",
      "=== epoch:167, train acc:0.59, test acc:0.4473 ===\n",
      "train loss:1.8416477989102098\n",
      "train loss:1.802560861209863\n",
      "train loss:1.8758282043386982\n",
      "=== epoch:168, train acc:0.58, test acc:0.4458 ===\n",
      "train loss:1.935509266212695\n",
      "train loss:1.8888294643025\n",
      "train loss:1.9096707044431847\n",
      "=== epoch:169, train acc:0.5833333333333334, test acc:0.4525 ===\n",
      "train loss:1.837957970989508\n",
      "train loss:1.9004012253401432\n",
      "train loss:1.9431657072142405\n",
      "=== epoch:170, train acc:0.5966666666666667, test acc:0.4564 ===\n",
      "train loss:1.7803272160191321\n",
      "train loss:1.8465863715260695\n",
      "train loss:1.8237802928057498\n",
      "=== epoch:171, train acc:0.6033333333333334, test acc:0.4595 ===\n",
      "train loss:1.8338942251612778\n",
      "train loss:1.7709471121981135\n",
      "train loss:1.8713284492819244\n",
      "=== epoch:172, train acc:0.6, test acc:0.4605 ===\n",
      "train loss:1.7657877291250346\n",
      "train loss:1.8039065953573605\n",
      "train loss:1.905092468850693\n",
      "=== epoch:173, train acc:0.5966666666666667, test acc:0.459 ===\n",
      "train loss:2.005975442365036\n",
      "train loss:1.8514464955011818\n",
      "train loss:1.8290978056620384\n",
      "=== epoch:174, train acc:0.6033333333333334, test acc:0.4633 ===\n",
      "train loss:1.867009181755707\n",
      "train loss:1.8742960574744947\n",
      "train loss:1.8209959021765234\n",
      "=== epoch:175, train acc:0.6133333333333333, test acc:0.4698 ===\n",
      "train loss:1.7152963280921656\n",
      "train loss:1.8335576317333107\n",
      "train loss:1.8105914096642841\n",
      "=== epoch:176, train acc:0.6166666666666667, test acc:0.47 ===\n",
      "train loss:1.8988170663933355\n",
      "train loss:1.7179447963872818\n",
      "train loss:1.845664294868906\n",
      "=== epoch:177, train acc:0.61, test acc:0.4676 ===\n",
      "train loss:1.7833341903242104\n",
      "train loss:1.7743211698366494\n",
      "train loss:1.8352302621341925\n",
      "=== epoch:178, train acc:0.5933333333333334, test acc:0.4633 ===\n",
      "train loss:1.8627432952543566\n",
      "train loss:1.7708718425273815\n",
      "train loss:1.8278716147395802\n",
      "=== epoch:179, train acc:0.6033333333333334, test acc:0.4648 ===\n",
      "train loss:1.7692188850431085\n",
      "train loss:1.838368293017939\n",
      "train loss:1.7992153561039836\n",
      "=== epoch:180, train acc:0.6166666666666667, test acc:0.4708 ===\n",
      "train loss:1.802685426035859\n",
      "train loss:1.7964365688360504\n",
      "train loss:1.8062054423856946\n",
      "=== epoch:181, train acc:0.62, test acc:0.4755 ===\n",
      "train loss:1.8189427501369828\n",
      "train loss:1.774728466800184\n",
      "train loss:1.7707192224338633\n",
      "=== epoch:182, train acc:0.6133333333333333, test acc:0.4726 ===\n",
      "train loss:1.756723592583256\n",
      "train loss:1.7670959790183114\n",
      "train loss:1.8999682311322914\n",
      "=== epoch:183, train acc:0.6266666666666667, test acc:0.4774 ===\n",
      "train loss:1.7748371105516292\n",
      "train loss:1.7534903070511942\n",
      "train loss:1.7695575082970811\n",
      "=== epoch:184, train acc:0.63, test acc:0.4811 ===\n",
      "train loss:1.6823269843873476\n",
      "train loss:1.8297221399719876\n",
      "train loss:1.8104778419284664\n",
      "=== epoch:185, train acc:0.6366666666666667, test acc:0.487 ===\n",
      "train loss:1.7664756889166315\n",
      "train loss:1.801245371994532\n",
      "train loss:1.699122413374132\n",
      "=== epoch:186, train acc:0.6333333333333333, test acc:0.4882 ===\n",
      "train loss:1.7743591767252038\n",
      "train loss:1.7581214883135186\n",
      "train loss:1.747711608750366\n",
      "=== epoch:187, train acc:0.6333333333333333, test acc:0.486 ===\n",
      "train loss:1.7959851879643625\n",
      "train loss:1.7091354042880473\n",
      "train loss:1.743305494101775\n",
      "=== epoch:188, train acc:0.63, test acc:0.4831 ===\n",
      "train loss:1.7556904392831434\n",
      "train loss:1.6683561983462036\n",
      "train loss:1.8023969353634826\n",
      "=== epoch:189, train acc:0.6333333333333333, test acc:0.4811 ===\n",
      "train loss:1.6785907294553162\n",
      "train loss:1.7407660735722081\n",
      "train loss:1.6639156295329918\n",
      "=== epoch:190, train acc:0.63, test acc:0.4822 ===\n",
      "train loss:1.7105688607718423\n",
      "train loss:1.702585848512995\n",
      "train loss:1.8020002871715664\n",
      "=== epoch:191, train acc:0.6233333333333333, test acc:0.4812 ===\n",
      "train loss:1.7700183589366612\n",
      "train loss:1.8454267225295726\n",
      "train loss:1.6753521619254537\n",
      "=== epoch:192, train acc:0.63, test acc:0.4838 ===\n",
      "train loss:1.7448129218830857\n",
      "train loss:1.817472527587749\n",
      "train loss:1.6977447136951431\n",
      "=== epoch:193, train acc:0.6466666666666666, test acc:0.4871 ===\n",
      "train loss:1.6811194829334954\n",
      "train loss:1.6913592833911657\n",
      "train loss:1.769716518854063\n",
      "=== epoch:194, train acc:0.64, test acc:0.4853 ===\n",
      "train loss:1.799623388634709\n",
      "train loss:1.7764948091546588\n",
      "train loss:1.7178530048419012\n",
      "=== epoch:195, train acc:0.65, test acc:0.4899 ===\n",
      "train loss:1.6610898112960928\n",
      "train loss:1.7360624831699132\n",
      "train loss:1.7798155483712583\n",
      "=== epoch:196, train acc:0.6466666666666666, test acc:0.4903 ===\n",
      "train loss:1.6901579518157603\n",
      "train loss:1.725700439311064\n",
      "train loss:1.68320289082782\n",
      "=== epoch:197, train acc:0.6366666666666667, test acc:0.4888 ===\n",
      "train loss:1.6860223042872966\n",
      "train loss:1.6840417431909358\n",
      "train loss:1.743675693854925\n",
      "=== epoch:198, train acc:0.6533333333333333, test acc:0.4924 ===\n",
      "train loss:1.7959246578320838\n",
      "train loss:1.7315603256920833\n",
      "train loss:1.682416795072471\n",
      "=== epoch:199, train acc:0.66, test acc:0.4966 ===\n",
      "train loss:1.6501997467616976\n",
      "train loss:1.7587793232915618\n",
      "train loss:1.7552568368220653\n",
      "=== epoch:200, train acc:0.6566666666666666, test acc:0.4998 ===\n",
      "train loss:1.726946578238295\n",
      "train loss:1.7152411614422232\n",
      "train loss:1.6974456104170839\n",
      "=== epoch:201, train acc:0.6666666666666666, test acc:0.5025 ===\n",
      "train loss:1.713315555496859\n",
      "train loss:1.760518062562367\n",
      "train loss:1.7520990292256153\n",
      "=== epoch:202, train acc:0.66, test acc:0.4987 ===\n",
      "train loss:1.7142990246313148\n",
      "train loss:1.7072291933278518\n",
      "train loss:1.6609436540880125\n",
      "=== epoch:203, train acc:0.66, test acc:0.5002 ===\n",
      "train loss:1.704629528989206\n",
      "train loss:1.6740190176503091\n",
      "train loss:1.618061825897196\n",
      "=== epoch:204, train acc:0.66, test acc:0.5009 ===\n",
      "train loss:1.6293839505355479\n",
      "train loss:1.6332448850003853\n",
      "train loss:1.662037633689406\n",
      "=== epoch:205, train acc:0.6466666666666666, test acc:0.4967 ===\n",
      "train loss:1.6244799193708166\n",
      "train loss:1.731152374684729\n",
      "train loss:1.6256100871639936\n",
      "=== epoch:206, train acc:0.65, test acc:0.4891 ===\n",
      "train loss:1.5351959772526274\n",
      "train loss:1.6236950552753735\n",
      "train loss:1.6574675220541442\n",
      "=== epoch:207, train acc:0.6466666666666666, test acc:0.4901 ===\n",
      "train loss:1.6467927905134632\n",
      "train loss:1.7536313897088136\n",
      "train loss:1.7434971258046246\n",
      "=== epoch:208, train acc:0.6533333333333333, test acc:0.4939 ===\n",
      "train loss:1.662855657238761\n",
      "train loss:1.6649481627049811\n",
      "train loss:1.5812052855918657\n",
      "=== epoch:209, train acc:0.6533333333333333, test acc:0.4973 ===\n",
      "train loss:1.5874574253086362\n",
      "train loss:1.684439764987443\n",
      "train loss:1.650480794346337\n",
      "=== epoch:210, train acc:0.6633333333333333, test acc:0.5007 ===\n",
      "train loss:1.6863494607700202\n",
      "train loss:1.5644317078103398\n",
      "train loss:1.5879166234643123\n",
      "=== epoch:211, train acc:0.6666666666666666, test acc:0.5033 ===\n",
      "train loss:1.5677335604206148\n",
      "train loss:1.583974486553399\n",
      "train loss:1.6259096504994568\n",
      "=== epoch:212, train acc:0.6666666666666666, test acc:0.5044 ===\n",
      "train loss:1.763337877072175\n",
      "train loss:1.7369819615396453\n",
      "train loss:1.6694020130568281\n",
      "=== epoch:213, train acc:0.66, test acc:0.5068 ===\n",
      "train loss:1.6905238654794175\n",
      "train loss:1.573170896519193\n",
      "train loss:1.4994133129555103\n",
      "=== epoch:214, train acc:0.66, test acc:0.5083 ===\n",
      "train loss:1.5890178586673795\n",
      "train loss:1.58338577171431\n",
      "train loss:1.6052805025238108\n",
      "=== epoch:215, train acc:0.66, test acc:0.5092 ===\n",
      "train loss:1.5948487139478271\n",
      "train loss:1.6478499159307631\n",
      "train loss:1.6267463484447324\n",
      "=== epoch:216, train acc:0.6666666666666666, test acc:0.5099 ===\n",
      "train loss:1.6097505133087464\n",
      "train loss:1.5527320526730173\n",
      "train loss:1.5608391958871541\n",
      "=== epoch:217, train acc:0.6633333333333333, test acc:0.508 ===\n",
      "train loss:1.5615123054261753\n",
      "train loss:1.6069389243860241\n",
      "train loss:1.5835267426907012\n",
      "=== epoch:218, train acc:0.66, test acc:0.5096 ===\n",
      "train loss:1.5710530049604725\n",
      "train loss:1.6087765489428245\n",
      "train loss:1.6427957058571885\n",
      "=== epoch:219, train acc:0.65, test acc:0.5065 ===\n",
      "train loss:1.516098277233332\n",
      "train loss:1.528173935581287\n",
      "train loss:1.6255550165941788\n",
      "=== epoch:220, train acc:0.65, test acc:0.5067 ===\n",
      "train loss:1.5783906092165572\n",
      "train loss:1.586522427591574\n",
      "train loss:1.543419201681643\n",
      "=== epoch:221, train acc:0.6433333333333333, test acc:0.5017 ===\n",
      "train loss:1.558061381150731\n",
      "train loss:1.5121792427456358\n",
      "train loss:1.548245008518345\n",
      "=== epoch:222, train acc:0.65, test acc:0.5047 ===\n",
      "train loss:1.4975506510574825\n",
      "train loss:1.53131943593079\n",
      "train loss:1.4804722694633299\n",
      "=== epoch:223, train acc:0.6466666666666666, test acc:0.5051 ===\n",
      "train loss:1.5462734645454086\n",
      "train loss:1.5205472084865366\n",
      "train loss:1.552639099128517\n",
      "=== epoch:224, train acc:0.65, test acc:0.5053 ===\n",
      "train loss:1.5652307676042596\n",
      "train loss:1.5279612268641432\n",
      "train loss:1.548787428068685\n",
      "=== epoch:225, train acc:0.6533333333333333, test acc:0.506 ===\n",
      "train loss:1.5104263222855383\n",
      "train loss:1.5251907865991599\n",
      "train loss:1.5705078631361653\n",
      "=== epoch:226, train acc:0.6466666666666666, test acc:0.5047 ===\n",
      "train loss:1.5476781006321423\n",
      "train loss:1.5704369169870624\n",
      "train loss:1.4979546288372665\n",
      "=== epoch:227, train acc:0.6533333333333333, test acc:0.5045 ===\n",
      "train loss:1.4272869930171475\n",
      "train loss:1.568939280635584\n",
      "train loss:1.517316025539789\n",
      "=== epoch:228, train acc:0.6533333333333333, test acc:0.5087 ===\n",
      "train loss:1.607822068286474\n",
      "train loss:1.5801021294356732\n",
      "train loss:1.4537604757592026\n",
      "=== epoch:229, train acc:0.6566666666666666, test acc:0.5194 ===\n",
      "train loss:1.5039703245776632\n",
      "train loss:1.51041662477909\n",
      "train loss:1.407981367811353\n",
      "=== epoch:230, train acc:0.6666666666666666, test acc:0.5245 ===\n",
      "train loss:1.5723674696853664\n",
      "train loss:1.5474547058133663\n",
      "train loss:1.4624590256824619\n",
      "=== epoch:231, train acc:0.6833333333333333, test acc:0.5278 ===\n",
      "train loss:1.676092934634474\n",
      "train loss:1.488610093380038\n",
      "train loss:1.5045187078520752\n",
      "=== epoch:232, train acc:0.6833333333333333, test acc:0.5245 ===\n",
      "train loss:1.382864602782996\n",
      "train loss:1.3647235479300952\n",
      "train loss:1.5124600742945489\n",
      "=== epoch:233, train acc:0.6733333333333333, test acc:0.5282 ===\n",
      "train loss:1.5447252955358162\n",
      "train loss:1.526799273921188\n",
      "train loss:1.3489914617678627\n",
      "=== epoch:234, train acc:0.6733333333333333, test acc:0.526 ===\n",
      "train loss:1.3882374849659809\n",
      "train loss:1.4971460469806892\n",
      "train loss:1.4985802445515461\n",
      "=== epoch:235, train acc:0.68, test acc:0.5295 ===\n",
      "train loss:1.48663530824493\n",
      "train loss:1.4825723167692224\n",
      "train loss:1.445367823055158\n",
      "=== epoch:236, train acc:0.6866666666666666, test acc:0.531 ===\n",
      "train loss:1.420152118897549\n",
      "train loss:1.4586538722631017\n",
      "train loss:1.4686156571924094\n",
      "=== epoch:237, train acc:0.6833333333333333, test acc:0.5247 ===\n",
      "train loss:1.5704228781466059\n",
      "train loss:1.4071579431647967\n",
      "train loss:1.4896676397847282\n",
      "=== epoch:238, train acc:0.6766666666666666, test acc:0.5235 ===\n",
      "train loss:1.5427810789642573\n",
      "train loss:1.3696719139326266\n",
      "train loss:1.4123529225934432\n",
      "=== epoch:239, train acc:0.68, test acc:0.5269 ===\n",
      "train loss:1.455432666548676\n",
      "train loss:1.5504160485764402\n",
      "train loss:1.49028291951026\n",
      "=== epoch:240, train acc:0.6833333333333333, test acc:0.5272 ===\n",
      "train loss:1.372702170837967\n",
      "train loss:1.5304781220902033\n",
      "train loss:1.4325907889159502\n",
      "=== epoch:241, train acc:0.68, test acc:0.5246 ===\n",
      "train loss:1.4897437090337489\n",
      "train loss:1.509164880091321\n",
      "train loss:1.425928372721869\n",
      "=== epoch:242, train acc:0.6833333333333333, test acc:0.527 ===\n",
      "train loss:1.502239539579878\n",
      "train loss:1.4533715698997625\n",
      "train loss:1.4550931899871573\n",
      "=== epoch:243, train acc:0.68, test acc:0.5281 ===\n",
      "train loss:1.3882571127931729\n",
      "train loss:1.6134039713735628\n",
      "train loss:1.4972551945141068\n",
      "=== epoch:244, train acc:0.6866666666666666, test acc:0.5286 ===\n",
      "train loss:1.448584345055082\n",
      "train loss:1.4146472196732878\n",
      "train loss:1.4467920966245362\n",
      "=== epoch:245, train acc:0.69, test acc:0.5289 ===\n",
      "train loss:1.4719508400762686\n",
      "train loss:1.4462649163361554\n",
      "train loss:1.4254463638796417\n",
      "=== epoch:246, train acc:0.69, test acc:0.5302 ===\n",
      "train loss:1.407069740431278\n",
      "train loss:1.4441999160105403\n",
      "train loss:1.463100741519047\n",
      "=== epoch:247, train acc:0.7, test acc:0.5346 ===\n",
      "train loss:1.3782543248097925\n",
      "train loss:1.326289426921329\n",
      "train loss:1.3168727387505714\n",
      "=== epoch:248, train acc:0.69, test acc:0.536 ===\n",
      "train loss:1.458688219134695\n",
      "train loss:1.4475266477858306\n",
      "train loss:1.3778161770574047\n",
      "=== epoch:249, train acc:0.6966666666666667, test acc:0.5386 ===\n",
      "train loss:1.4462242150198796\n",
      "train loss:1.4295055338767102\n",
      "train loss:1.384929987434464\n",
      "=== epoch:250, train acc:0.7066666666666667, test acc:0.5423 ===\n",
      "train loss:1.4856975939357722\n",
      "train loss:1.4209607315409323\n",
      "train loss:1.5085253802311098\n",
      "=== epoch:251, train acc:0.7133333333333334, test acc:0.5442 ===\n",
      "train loss:1.2767498421376475\n",
      "train loss:1.4050512613689892\n",
      "train loss:1.2894225298548077\n",
      "=== epoch:252, train acc:0.7066666666666667, test acc:0.5422 ===\n",
      "train loss:1.443188613294693\n",
      "train loss:1.4399547738084322\n",
      "train loss:1.355869152789519\n",
      "=== epoch:253, train acc:0.7066666666666667, test acc:0.5418 ===\n",
      "train loss:1.4057173396019464\n",
      "train loss:1.4034862825102925\n",
      "train loss:1.2809648485974616\n",
      "=== epoch:254, train acc:0.6933333333333334, test acc:0.5411 ===\n",
      "train loss:1.362314803887405\n",
      "train loss:1.351186724069764\n",
      "train loss:1.3692587806795509\n",
      "=== epoch:255, train acc:0.6966666666666667, test acc:0.542 ===\n",
      "train loss:1.485838075147093\n",
      "train loss:1.2770698380564125\n",
      "train loss:1.5636327757951125\n",
      "=== epoch:256, train acc:0.7, test acc:0.5457 ===\n",
      "train loss:1.1815775461429459\n",
      "train loss:1.3057812126738677\n",
      "train loss:1.2180819977039816\n",
      "=== epoch:257, train acc:0.7066666666666667, test acc:0.5433 ===\n",
      "train loss:1.4106180738927774\n",
      "train loss:1.387572548507743\n",
      "train loss:1.3034715963661856\n",
      "=== epoch:258, train acc:0.7066666666666667, test acc:0.5434 ===\n",
      "train loss:1.3144666172308974\n",
      "train loss:1.340911254991516\n",
      "train loss:1.2711627272157207\n",
      "=== epoch:259, train acc:0.7066666666666667, test acc:0.5448 ===\n",
      "train loss:1.309719041283934\n",
      "train loss:1.3517430322232105\n",
      "train loss:1.41965349815028\n",
      "=== epoch:260, train acc:0.7033333333333334, test acc:0.545 ===\n",
      "train loss:1.3964976521621717\n",
      "train loss:1.3138812813598992\n",
      "train loss:1.2772419784493596\n",
      "=== epoch:261, train acc:0.7066666666666667, test acc:0.549 ===\n",
      "train loss:1.2800947290342897\n",
      "train loss:1.252048424502078\n",
      "train loss:1.3157030183742793\n",
      "=== epoch:262, train acc:0.7066666666666667, test acc:0.5519 ===\n",
      "train loss:1.4380360996158452\n",
      "train loss:1.2929381108723026\n",
      "train loss:1.341789194045111\n",
      "=== epoch:263, train acc:0.7033333333333334, test acc:0.5533 ===\n",
      "train loss:1.4114524918362803\n",
      "train loss:1.2613277691870317\n",
      "train loss:1.268674277141944\n",
      "=== epoch:264, train acc:0.71, test acc:0.5519 ===\n",
      "train loss:1.454822532885893\n",
      "train loss:1.338680216648951\n",
      "train loss:1.32015814979293\n",
      "=== epoch:265, train acc:0.7133333333333334, test acc:0.5526 ===\n",
      "train loss:1.2970828342094969\n",
      "train loss:1.3823508133021527\n",
      "train loss:1.3079102658831647\n",
      "=== epoch:266, train acc:0.71, test acc:0.5537 ===\n",
      "train loss:1.3111605385070753\n",
      "train loss:1.4997855150613966\n",
      "train loss:1.339331547134036\n",
      "=== epoch:267, train acc:0.71, test acc:0.5523 ===\n",
      "train loss:1.235335572619871\n",
      "train loss:1.3584439061875784\n",
      "train loss:1.2917441393770794\n",
      "=== epoch:268, train acc:0.71, test acc:0.5511 ===\n",
      "train loss:1.2433001587533743\n",
      "train loss:1.2347949302151549\n",
      "train loss:1.2687115258062993\n",
      "=== epoch:269, train acc:0.7033333333333334, test acc:0.5486 ===\n",
      "train loss:1.3862028365890575\n",
      "train loss:1.2981685153971094\n",
      "train loss:1.3505470113014633\n",
      "=== epoch:270, train acc:0.7066666666666667, test acc:0.5466 ===\n",
      "train loss:1.1551343038131412\n",
      "train loss:1.2126959882779689\n",
      "train loss:1.3865002709891454\n",
      "=== epoch:271, train acc:0.71, test acc:0.5506 ===\n",
      "train loss:1.2223672436075839\n",
      "train loss:1.343674232405865\n",
      "train loss:1.2396829825951192\n",
      "=== epoch:272, train acc:0.7033333333333334, test acc:0.5535 ===\n",
      "train loss:1.2611528509839196\n",
      "train loss:1.382999375451818\n",
      "train loss:1.3136759240342406\n",
      "=== epoch:273, train acc:0.7066666666666667, test acc:0.5519 ===\n",
      "train loss:1.2200142151864959\n",
      "train loss:1.1000370551547571\n",
      "train loss:1.1899119319475078\n",
      "=== epoch:274, train acc:0.7133333333333334, test acc:0.555 ===\n",
      "train loss:1.3285686527147238\n",
      "train loss:1.3614894590309417\n",
      "train loss:1.3131463285328722\n",
      "=== epoch:275, train acc:0.71, test acc:0.5572 ===\n",
      "train loss:1.3159548447455267\n",
      "train loss:1.2671792052367066\n",
      "train loss:1.1605056372514682\n",
      "=== epoch:276, train acc:0.7033333333333334, test acc:0.5542 ===\n",
      "train loss:1.1174307814084323\n",
      "train loss:1.175832189053279\n",
      "train loss:1.1869807998077422\n",
      "=== epoch:277, train acc:0.72, test acc:0.5597 ===\n",
      "train loss:1.17026279173577\n",
      "train loss:1.2623865299045163\n",
      "train loss:1.1975359022270837\n",
      "=== epoch:278, train acc:0.72, test acc:0.5629 ===\n",
      "train loss:1.3068223840883524\n",
      "train loss:1.2699090095714944\n",
      "train loss:1.126867194098558\n",
      "=== epoch:279, train acc:0.7266666666666667, test acc:0.5602 ===\n",
      "train loss:1.1902690146000945\n",
      "train loss:1.3054200457060707\n",
      "train loss:1.3854875162621547\n",
      "=== epoch:280, train acc:0.7166666666666667, test acc:0.5563 ===\n",
      "train loss:1.1898673191101448\n",
      "train loss:1.2486389646371159\n",
      "train loss:1.2120561724094956\n",
      "=== epoch:281, train acc:0.72, test acc:0.5549 ===\n",
      "train loss:1.16735122946088\n",
      "train loss:1.12762973334805\n",
      "train loss:1.2232237911556911\n",
      "=== epoch:282, train acc:0.7166666666666667, test acc:0.5569 ===\n",
      "train loss:1.2465011813784657\n",
      "train loss:1.195208149772879\n",
      "train loss:1.2793634054230345\n",
      "=== epoch:283, train acc:0.7166666666666667, test acc:0.5567 ===\n",
      "train loss:1.1682245326808323\n",
      "train loss:1.211942162225217\n",
      "train loss:1.3110050714151\n",
      "=== epoch:284, train acc:0.7233333333333334, test acc:0.5661 ===\n",
      "train loss:1.2709750314351296\n",
      "train loss:1.2281261910386219\n",
      "train loss:1.2159275167531853\n",
      "=== epoch:285, train acc:0.7233333333333334, test acc:0.5652 ===\n",
      "train loss:1.162744067959069\n",
      "train loss:1.0834162430812797\n",
      "train loss:1.2217896798561863\n",
      "=== epoch:286, train acc:0.72, test acc:0.5664 ===\n",
      "train loss:1.1203817159313163\n",
      "train loss:1.0757025959546014\n",
      "train loss:1.2396025282427063\n",
      "=== epoch:287, train acc:0.73, test acc:0.5696 ===\n",
      "train loss:1.174307112617322\n",
      "train loss:1.1139732649339957\n",
      "train loss:1.1815305811670245\n",
      "=== epoch:288, train acc:0.7266666666666667, test acc:0.5706 ===\n",
      "train loss:1.2000663647036554\n",
      "train loss:1.1830498777304337\n",
      "train loss:1.2710396539502096\n",
      "=== epoch:289, train acc:0.7366666666666667, test acc:0.5757 ===\n",
      "train loss:1.0501458870532914\n",
      "train loss:1.2299274792559107\n",
      "train loss:1.15689995056496\n",
      "=== epoch:290, train acc:0.74, test acc:0.5746 ===\n",
      "train loss:1.1732327576251202\n",
      "train loss:1.1091655939872818\n",
      "train loss:1.2005382558746016\n",
      "=== epoch:291, train acc:0.7333333333333333, test acc:0.5742 ===\n",
      "train loss:1.2068622781991485\n",
      "train loss:1.1140014468891026\n",
      "train loss:1.087283455929615\n",
      "=== epoch:292, train acc:0.7366666666666667, test acc:0.5756 ===\n",
      "train loss:1.0753889633779223\n",
      "train loss:1.1957056398005879\n",
      "train loss:1.133464849636295\n",
      "=== epoch:293, train acc:0.7366666666666667, test acc:0.5726 ===\n",
      "train loss:1.154363894685735\n",
      "train loss:1.1730951232570006\n",
      "train loss:1.2126234644280955\n",
      "=== epoch:294, train acc:0.7366666666666667, test acc:0.5729 ===\n",
      "train loss:1.0451676557471707\n",
      "train loss:1.1577195603184958\n",
      "train loss:1.1303783620898802\n",
      "=== epoch:295, train acc:0.7366666666666667, test acc:0.5791 ===\n",
      "train loss:1.1711270989062368\n",
      "train loss:1.1098423041813006\n",
      "train loss:1.1467770294311346\n",
      "=== epoch:296, train acc:0.7466666666666667, test acc:0.5809 ===\n",
      "train loss:1.230871076949316\n",
      "train loss:1.1294801029564383\n",
      "train loss:1.1468926645186144\n",
      "=== epoch:297, train acc:0.7466666666666667, test acc:0.5826 ===\n",
      "train loss:1.2478419577638318\n",
      "train loss:1.203169603856519\n",
      "train loss:1.078446264137522\n",
      "=== epoch:298, train acc:0.7366666666666667, test acc:0.579 ===\n",
      "train loss:1.1901736726673366\n",
      "train loss:1.148501525433414\n",
      "train loss:1.0817690891592298\n",
      "=== epoch:299, train acc:0.7466666666666667, test acc:0.5845 ===\n",
      "train loss:1.0001549353999422\n",
      "train loss:1.1015949571280714\n",
      "train loss:0.983387949140387\n",
      "=== epoch:300, train acc:0.7366666666666667, test acc:0.5782 ===\n",
      "train loss:1.2270835028381728\n",
      "train loss:1.1685053340747833\n",
      "train loss:1.0395749723166439\n",
      "=== epoch:301, train acc:0.7433333333333333, test acc:0.5828 ===\n",
      "train loss:0.9309385321289452\n",
      "train loss:0.9882776125109014\n",
      "train loss:1.1022679816882501\n",
      "=== epoch:302, train acc:0.7433333333333333, test acc:0.5791 ===\n",
      "train loss:1.1492162655785965\n",
      "train loss:1.157053553167644\n",
      "train loss:0.9142673000097611\n",
      "=== epoch:303, train acc:0.7466666666666667, test acc:0.5831 ===\n",
      "train loss:1.0545514014023427\n",
      "train loss:1.1293282297137404\n",
      "train loss:1.0956504648575875\n",
      "=== epoch:304, train acc:0.7433333333333333, test acc:0.5868 ===\n",
      "train loss:1.0302977480616293\n",
      "train loss:1.2286525463058495\n",
      "train loss:1.131815186733775\n",
      "=== epoch:305, train acc:0.76, test acc:0.5891 ===\n",
      "train loss:1.0754023608688437\n",
      "train loss:1.1058783324333705\n",
      "train loss:0.9923971901240829\n",
      "=== epoch:306, train acc:0.7466666666666667, test acc:0.5867 ===\n",
      "train loss:1.1212712826028732\n",
      "train loss:1.08383167722999\n",
      "train loss:1.058519397902431\n",
      "=== epoch:307, train acc:0.7533333333333333, test acc:0.5878 ===\n",
      "train loss:1.0932276637058636\n",
      "train loss:0.9854133191758554\n",
      "train loss:1.1527064102382094\n",
      "=== epoch:308, train acc:0.77, test acc:0.5917 ===\n",
      "train loss:1.050061646258114\n",
      "train loss:1.0435584626641223\n",
      "train loss:1.126046525946667\n",
      "=== epoch:309, train acc:0.75, test acc:0.5887 ===\n",
      "train loss:1.07797444932373\n",
      "train loss:1.066061123632768\n",
      "train loss:1.134813544804564\n",
      "=== epoch:310, train acc:0.7566666666666667, test acc:0.5931 ===\n",
      "train loss:1.1823676031440313\n",
      "train loss:1.0092382690142336\n",
      "train loss:0.9742615963289092\n",
      "=== epoch:311, train acc:0.7666666666666667, test acc:0.592 ===\n",
      "train loss:1.112518248293783\n",
      "train loss:0.9684142760521443\n",
      "train loss:0.9707523440047638\n",
      "=== epoch:312, train acc:0.7666666666666667, test acc:0.5937 ===\n",
      "train loss:0.9845548293086256\n",
      "train loss:1.1048116598825228\n",
      "train loss:1.1126060284403942\n",
      "=== epoch:313, train acc:0.7633333333333333, test acc:0.5935 ===\n",
      "train loss:1.0863087559914208\n",
      "train loss:1.1115956076782632\n",
      "train loss:1.0037693744513987\n",
      "=== epoch:314, train acc:0.7733333333333333, test acc:0.5987 ===\n",
      "train loss:1.0897108510356808\n",
      "train loss:0.8511887132782932\n",
      "train loss:1.017112655623485\n",
      "=== epoch:315, train acc:0.7766666666666666, test acc:0.5988 ===\n",
      "train loss:1.047677393914599\n",
      "train loss:0.9588419931198391\n",
      "train loss:1.0653116807709033\n",
      "=== epoch:316, train acc:0.77, test acc:0.5996 ===\n",
      "train loss:1.0668133765784162\n",
      "train loss:0.905498686629705\n",
      "train loss:0.8835698608080577\n",
      "=== epoch:317, train acc:0.78, test acc:0.6022 ===\n",
      "train loss:1.003992099295725\n",
      "train loss:1.0632318884116903\n",
      "train loss:1.0425483057698606\n",
      "=== epoch:318, train acc:0.7733333333333333, test acc:0.6081 ===\n",
      "train loss:1.052811163635062\n",
      "train loss:0.996434208337343\n",
      "train loss:1.0838518901383087\n",
      "=== epoch:319, train acc:0.78, test acc:0.6064 ===\n",
      "train loss:0.9493705218453548\n",
      "train loss:1.0834019045245413\n",
      "train loss:1.0436159151355338\n",
      "=== epoch:320, train acc:0.78, test acc:0.6097 ===\n",
      "train loss:1.1264103785849329\n",
      "train loss:0.8655890288366176\n",
      "train loss:0.908456917444632\n",
      "=== epoch:321, train acc:0.7833333333333333, test acc:0.6066 ===\n",
      "train loss:0.9884984733421597\n",
      "train loss:0.9897965277402371\n",
      "train loss:1.0521886674218695\n",
      "=== epoch:322, train acc:0.78, test acc:0.604 ===\n",
      "train loss:1.0001815048347602\n",
      "train loss:1.0202248205338489\n",
      "train loss:0.9655851143511156\n",
      "=== epoch:323, train acc:0.7866666666666666, test acc:0.6066 ===\n",
      "train loss:0.9167122385867752\n",
      "train loss:0.9414341066386459\n",
      "train loss:1.0136144112252774\n",
      "=== epoch:324, train acc:0.7866666666666666, test acc:0.606 ===\n",
      "train loss:0.9490414188543057\n",
      "train loss:0.9991632562710977\n",
      "train loss:0.9543417029146485\n",
      "=== epoch:325, train acc:0.78, test acc:0.6036 ===\n",
      "train loss:1.0259929138198627\n",
      "train loss:0.9722297559068512\n",
      "train loss:1.0445626486227433\n",
      "=== epoch:326, train acc:0.7933333333333333, test acc:0.6051 ===\n",
      "train loss:0.9599517422963768\n",
      "train loss:0.9441551134695746\n",
      "train loss:0.8838843242458866\n",
      "=== epoch:327, train acc:0.79, test acc:0.607 ===\n",
      "train loss:0.908785865927497\n",
      "train loss:0.9158773153171823\n",
      "train loss:0.8313638154746731\n",
      "=== epoch:328, train acc:0.79, test acc:0.608 ===\n",
      "train loss:1.055161692701553\n",
      "train loss:0.963040559430871\n",
      "train loss:0.9669999391799218\n",
      "=== epoch:329, train acc:0.79, test acc:0.6119 ===\n",
      "train loss:1.0218601977666593\n",
      "train loss:1.0112470717678386\n",
      "train loss:0.9124062737272015\n",
      "=== epoch:330, train acc:0.7933333333333333, test acc:0.6121 ===\n",
      "train loss:0.9833448401260153\n",
      "train loss:0.9775471938579605\n",
      "train loss:0.9653342105491322\n",
      "=== epoch:331, train acc:0.7966666666666666, test acc:0.615 ===\n",
      "train loss:0.8959895570285439\n",
      "train loss:0.9665986642786918\n",
      "train loss:0.9464833286316191\n",
      "=== epoch:332, train acc:0.7933333333333333, test acc:0.6086 ===\n",
      "train loss:0.8298559162541944\n",
      "train loss:0.9249684057364844\n",
      "train loss:1.0454739879787542\n",
      "=== epoch:333, train acc:0.7933333333333333, test acc:0.61 ===\n",
      "train loss:0.9258419412889799\n",
      "train loss:0.8389788675314337\n",
      "train loss:0.9593999067473105\n",
      "=== epoch:334, train acc:0.7933333333333333, test acc:0.6145 ===\n",
      "train loss:0.9271335961791358\n",
      "train loss:0.9594749874010108\n",
      "train loss:1.0474108082393623\n",
      "=== epoch:335, train acc:0.8, test acc:0.6169 ===\n",
      "train loss:0.878232526970693\n",
      "train loss:0.9881454027270952\n",
      "train loss:0.9950298439796381\n",
      "=== epoch:336, train acc:0.8, test acc:0.6177 ===\n",
      "train loss:0.8188113441172732\n",
      "train loss:0.9614126684187917\n",
      "train loss:0.9030903818703306\n",
      "=== epoch:337, train acc:0.8, test acc:0.614 ===\n",
      "train loss:0.9374566940950637\n",
      "train loss:0.948614281560276\n",
      "train loss:0.7623422277545615\n",
      "=== epoch:338, train acc:0.8, test acc:0.6166 ===\n",
      "train loss:0.937291628230217\n",
      "train loss:0.8951579643100881\n",
      "train loss:0.8149574057947517\n",
      "=== epoch:339, train acc:0.8, test acc:0.6124 ===\n",
      "train loss:0.8452644339760811\n",
      "train loss:0.8508252054215574\n",
      "train loss:0.9062530444904118\n",
      "=== epoch:340, train acc:0.8, test acc:0.6131 ===\n",
      "train loss:1.0262141890391676\n",
      "train loss:0.9009554102508389\n",
      "train loss:0.9386514435682064\n",
      "=== epoch:341, train acc:0.8, test acc:0.6156 ===\n",
      "train loss:0.8730492943346156\n",
      "train loss:0.9300420820597323\n",
      "train loss:0.9451783079681358\n",
      "=== epoch:342, train acc:0.8033333333333333, test acc:0.617 ===\n",
      "train loss:1.030064168671688\n",
      "train loss:0.941757691975262\n",
      "train loss:0.8680035048488679\n",
      "=== epoch:343, train acc:0.8, test acc:0.6127 ===\n",
      "train loss:0.828931074393036\n",
      "train loss:0.9677978641591609\n",
      "train loss:0.8802002137267073\n",
      "=== epoch:344, train acc:0.8066666666666666, test acc:0.6261 ===\n",
      "train loss:0.8394936668465135\n",
      "train loss:0.961842071347525\n",
      "train loss:0.8185398080481022\n",
      "=== epoch:345, train acc:0.8033333333333333, test acc:0.6207 ===\n",
      "train loss:0.8585225880988331\n",
      "train loss:0.932485305642262\n",
      "train loss:0.8128441584402298\n",
      "=== epoch:346, train acc:0.8066666666666666, test acc:0.6249 ===\n",
      "train loss:0.805230732291904\n",
      "train loss:0.8828723785312779\n",
      "train loss:0.9316246603888537\n",
      "=== epoch:347, train acc:0.81, test acc:0.6265 ===\n",
      "train loss:0.8098247316125021\n",
      "train loss:0.8901810318192146\n",
      "train loss:0.8075586752444712\n",
      "=== epoch:348, train acc:0.8066666666666666, test acc:0.6295 ===\n",
      "train loss:0.8690701552903555\n",
      "train loss:0.83036204427481\n",
      "train loss:0.7401232174831667\n",
      "=== epoch:349, train acc:0.81, test acc:0.629 ===\n",
      "train loss:0.8744567182100518\n",
      "train loss:0.8713732942617114\n",
      "train loss:0.8540135990996499\n",
      "=== epoch:350, train acc:0.81, test acc:0.6285 ===\n",
      "train loss:0.7511833471225111\n",
      "train loss:0.8767474876793087\n",
      "train loss:0.9492690033611133\n",
      "=== epoch:351, train acc:0.81, test acc:0.6275 ===\n",
      "train loss:0.8597194576763281\n",
      "train loss:0.8406975551074056\n",
      "train loss:0.8855085291881558\n",
      "=== epoch:352, train acc:0.81, test acc:0.627 ===\n",
      "train loss:0.7579176465866574\n",
      "train loss:0.8248881122671982\n",
      "train loss:0.8305442461534696\n",
      "=== epoch:353, train acc:0.8066666666666666, test acc:0.6255 ===\n",
      "train loss:0.7783287851040107\n",
      "train loss:0.6843438930507055\n",
      "train loss:0.9861767118574438\n",
      "=== epoch:354, train acc:0.8066666666666666, test acc:0.6274 ===\n",
      "train loss:0.8179778765609801\n",
      "train loss:0.886796555411746\n",
      "train loss:0.833479256838996\n",
      "=== epoch:355, train acc:0.81, test acc:0.6306 ===\n",
      "train loss:0.8025442153849898\n",
      "train loss:0.9278196449551913\n",
      "train loss:0.8870220715677157\n",
      "=== epoch:356, train acc:0.8033333333333333, test acc:0.6258 ===\n",
      "train loss:0.7458764349349226\n",
      "train loss:0.7822106053865535\n",
      "train loss:0.9179299622788419\n",
      "=== epoch:357, train acc:0.81, test acc:0.6288 ===\n",
      "train loss:0.7482467405271249\n",
      "train loss:0.7589318119148384\n",
      "train loss:0.7806384334557529\n",
      "=== epoch:358, train acc:0.81, test acc:0.6291 ===\n",
      "train loss:0.7107160602884101\n",
      "train loss:0.8475803203855788\n",
      "train loss:0.9041146212112094\n",
      "=== epoch:359, train acc:0.81, test acc:0.6295 ===\n",
      "train loss:0.946795586435534\n",
      "train loss:0.8126326406743487\n",
      "train loss:0.8633746493015358\n",
      "=== epoch:360, train acc:0.8066666666666666, test acc:0.63 ===\n",
      "train loss:0.8172945329766503\n",
      "train loss:0.8057294957278933\n",
      "train loss:0.8596556193416334\n",
      "=== epoch:361, train acc:0.8066666666666666, test acc:0.6304 ===\n",
      "train loss:0.817747490431542\n",
      "train loss:0.7816346131681665\n",
      "train loss:0.7690883996605491\n",
      "=== epoch:362, train acc:0.8066666666666666, test acc:0.6286 ===\n",
      "train loss:0.7741420205729953\n",
      "train loss:0.8273129046782719\n",
      "train loss:0.8117230460343741\n",
      "=== epoch:363, train acc:0.81, test acc:0.6325 ===\n",
      "train loss:0.7230641141488964\n",
      "train loss:0.7618748261554177\n",
      "train loss:0.7234088208302577\n",
      "=== epoch:364, train acc:0.81, test acc:0.6388 ===\n",
      "train loss:0.7654986119907018\n",
      "train loss:0.76811163144282\n",
      "train loss:0.7568330604572858\n",
      "=== epoch:365, train acc:0.8166666666666667, test acc:0.6359 ===\n",
      "train loss:0.6540465993607164\n",
      "train loss:0.7807691826449963\n",
      "train loss:0.7782301813413643\n",
      "=== epoch:366, train acc:0.8133333333333334, test acc:0.6372 ===\n",
      "train loss:0.8404181890188812\n",
      "train loss:0.860858078403695\n",
      "train loss:0.885038535674691\n",
      "=== epoch:367, train acc:0.8133333333333334, test acc:0.6383 ===\n",
      "train loss:0.8107591330016151\n",
      "train loss:0.8505890496950311\n",
      "train loss:0.7569898498303231\n",
      "=== epoch:368, train acc:0.8166666666666667, test acc:0.6385 ===\n",
      "train loss:0.6125097959053333\n",
      "train loss:0.6424706017779743\n",
      "train loss:0.8456380060192717\n",
      "=== epoch:369, train acc:0.8166666666666667, test acc:0.6411 ===\n",
      "train loss:0.6378542084452503\n",
      "train loss:0.8186202284663409\n",
      "train loss:0.7981874196087694\n",
      "=== epoch:370, train acc:0.8133333333333334, test acc:0.6421 ===\n",
      "train loss:0.7588165720377082\n",
      "train loss:0.7762696526103858\n",
      "train loss:0.8899833421563388\n",
      "=== epoch:371, train acc:0.8133333333333334, test acc:0.6384 ===\n",
      "train loss:0.6626067290428419\n",
      "train loss:0.6530270348155068\n",
      "train loss:0.7712139288072484\n",
      "=== epoch:372, train acc:0.8166666666666667, test acc:0.6389 ===\n",
      "train loss:0.7961811928574503\n",
      "train loss:0.7548484025284904\n",
      "train loss:0.8310607348796961\n",
      "=== epoch:373, train acc:0.82, test acc:0.6386 ===\n",
      "train loss:0.8749565884296614\n",
      "train loss:0.7204211313209306\n",
      "train loss:0.8203216175893349\n",
      "=== epoch:374, train acc:0.8133333333333334, test acc:0.6396 ===\n",
      "train loss:0.8139138712736641\n",
      "train loss:0.6439155758020586\n",
      "train loss:0.7744678240639427\n",
      "=== epoch:375, train acc:0.82, test acc:0.6453 ===\n",
      "train loss:0.7353171612897121\n",
      "train loss:0.6202793622602311\n",
      "train loss:0.7183842803381566\n",
      "=== epoch:376, train acc:0.8166666666666667, test acc:0.6486 ===\n",
      "train loss:0.6695784307121433\n",
      "train loss:0.6192340386247673\n",
      "train loss:0.7324838389046608\n",
      "=== epoch:377, train acc:0.8166666666666667, test acc:0.6509 ===\n",
      "train loss:0.7696529417922878\n",
      "train loss:0.6045965247210278\n",
      "train loss:0.6551446766229426\n",
      "=== epoch:378, train acc:0.8233333333333334, test acc:0.6518 ===\n",
      "train loss:0.5898017317897272\n",
      "train loss:0.8328235805562767\n",
      "train loss:0.7205709048002323\n",
      "=== epoch:379, train acc:0.8233333333333334, test acc:0.6538 ===\n",
      "train loss:0.7754955348845013\n",
      "train loss:0.7504213772347402\n",
      "train loss:0.7638363629891988\n",
      "=== epoch:380, train acc:0.8333333333333334, test acc:0.6533 ===\n",
      "train loss:0.6691069872176106\n",
      "train loss:0.7233517805933216\n",
      "train loss:0.6596124854858095\n",
      "=== epoch:381, train acc:0.83, test acc:0.6519 ===\n",
      "train loss:0.6256101283162596\n",
      "train loss:0.702783001351914\n",
      "train loss:0.6475517318674886\n",
      "=== epoch:382, train acc:0.8333333333333334, test acc:0.6518 ===\n",
      "train loss:0.7187812501996477\n",
      "train loss:0.5587371246603615\n",
      "train loss:0.6323413203833962\n",
      "=== epoch:383, train acc:0.8333333333333334, test acc:0.6508 ===\n",
      "train loss:0.6878727901184111\n",
      "train loss:0.7752982402760006\n",
      "train loss:0.7428669267604948\n",
      "=== epoch:384, train acc:0.8333333333333334, test acc:0.6527 ===\n",
      "train loss:0.7173197385637892\n",
      "train loss:0.6666144772548305\n",
      "train loss:0.6354622666020928\n",
      "=== epoch:385, train acc:0.8366666666666667, test acc:0.6549 ===\n",
      "train loss:0.5636322741301201\n",
      "train loss:0.6597478092268146\n",
      "train loss:0.6430124758688041\n",
      "=== epoch:386, train acc:0.8333333333333334, test acc:0.6488 ===\n",
      "train loss:0.586910343316072\n",
      "train loss:0.7033824740380921\n",
      "train loss:0.7595735074856914\n",
      "=== epoch:387, train acc:0.8366666666666667, test acc:0.6497 ===\n",
      "train loss:0.5896226677197747\n",
      "train loss:0.7406139106948116\n",
      "train loss:0.612583594905071\n",
      "=== epoch:388, train acc:0.8366666666666667, test acc:0.6527 ===\n",
      "train loss:0.704152017557297\n",
      "train loss:0.6440135366597926\n",
      "train loss:0.6555082114410132\n",
      "=== epoch:389, train acc:0.8366666666666667, test acc:0.6542 ===\n",
      "train loss:0.6158369440959187\n",
      "train loss:0.637903422651668\n",
      "train loss:0.6442347247123419\n",
      "=== epoch:390, train acc:0.84, test acc:0.6545 ===\n",
      "train loss:0.7323265486310848\n",
      "train loss:0.6750526046662975\n",
      "train loss:0.574243991923082\n",
      "=== epoch:391, train acc:0.8366666666666667, test acc:0.6527 ===\n",
      "train loss:0.6062488634994561\n",
      "train loss:0.6297567331292417\n",
      "train loss:0.6499031144567569\n",
      "=== epoch:392, train acc:0.8333333333333334, test acc:0.6562 ===\n",
      "train loss:0.6407932796363011\n",
      "train loss:0.7215011316681067\n",
      "train loss:0.6948249634381973\n",
      "=== epoch:393, train acc:0.84, test acc:0.6592 ===\n",
      "train loss:0.6706944916655792\n",
      "train loss:0.6822361737455667\n",
      "train loss:0.6684237567722487\n",
      "=== epoch:394, train acc:0.8466666666666667, test acc:0.6593 ===\n",
      "train loss:0.5524094966063352\n",
      "train loss:0.6332952957378396\n",
      "train loss:0.6259014471071365\n",
      "=== epoch:395, train acc:0.85, test acc:0.6603 ===\n",
      "train loss:0.746594300488205\n",
      "train loss:0.5973800188744327\n",
      "train loss:0.7775423440750565\n",
      "=== epoch:396, train acc:0.8466666666666667, test acc:0.6599 ===\n",
      "train loss:0.6083119278398633\n",
      "train loss:0.8775030797913399\n",
      "train loss:0.668220322818339\n",
      "=== epoch:397, train acc:0.8533333333333334, test acc:0.6603 ===\n",
      "train loss:0.6833204032508905\n",
      "train loss:0.518685871106819\n",
      "train loss:0.6733598512706166\n",
      "=== epoch:398, train acc:0.8533333333333334, test acc:0.6629 ===\n",
      "train loss:0.6535531235558626\n",
      "train loss:0.7284699575670314\n",
      "train loss:0.6223554644440141\n",
      "=== epoch:399, train acc:0.8533333333333334, test acc:0.6641 ===\n",
      "train loss:0.7781371973079899\n",
      "train loss:0.6150971035181737\n",
      "train loss:0.6079249455714276\n",
      "=== epoch:400, train acc:0.85, test acc:0.6653 ===\n",
      "train loss:0.6555537689144874\n",
      "train loss:0.6188170827854221\n",
      "train loss:0.6240635862990387\n",
      "=== epoch:401, train acc:0.8533333333333334, test acc:0.6666 ===\n",
      "train loss:0.6271234732106764\n",
      "train loss:0.5047927968895869\n",
      "train loss:0.6946882629180611\n",
      "=== epoch:402, train acc:0.85, test acc:0.6647 ===\n",
      "train loss:0.5087558582762025\n",
      "train loss:0.6333209779263868\n",
      "train loss:0.6624358303599148\n",
      "=== epoch:403, train acc:0.8466666666666667, test acc:0.664 ===\n",
      "train loss:0.6676324603499917\n",
      "train loss:0.5938434765749872\n",
      "train loss:0.7191672555042302\n",
      "=== epoch:404, train acc:0.8466666666666667, test acc:0.6636 ===\n",
      "train loss:0.5738833330792386\n",
      "train loss:0.5820923188478266\n",
      "train loss:0.5676164462402569\n",
      "=== epoch:405, train acc:0.85, test acc:0.6653 ===\n",
      "train loss:0.6231310036349352\n",
      "train loss:0.5188734487672072\n",
      "train loss:0.5492025224702671\n",
      "=== epoch:406, train acc:0.8533333333333334, test acc:0.6656 ===\n",
      "train loss:0.6087058076118856\n",
      "train loss:0.6835900630884566\n",
      "train loss:0.519672578750249\n",
      "=== epoch:407, train acc:0.85, test acc:0.6655 ===\n",
      "train loss:0.6458374565481526\n",
      "train loss:0.7653711547813754\n",
      "train loss:0.6624120300500081\n",
      "=== epoch:408, train acc:0.8566666666666667, test acc:0.6681 ===\n",
      "train loss:0.5446428765631385\n",
      "train loss:0.6879534076173648\n",
      "train loss:0.5364690820020523\n",
      "=== epoch:409, train acc:0.8533333333333334, test acc:0.6722 ===\n",
      "train loss:0.6314377949561003\n",
      "train loss:0.6503319757384743\n",
      "train loss:0.5987113647054266\n",
      "=== epoch:410, train acc:0.86, test acc:0.672 ===\n",
      "train loss:0.6124643406862973\n",
      "train loss:0.5384923657454138\n",
      "train loss:0.5384047579249013\n",
      "=== epoch:411, train acc:0.8633333333333333, test acc:0.6708 ===\n",
      "train loss:0.5468571482874701\n",
      "train loss:0.6211154106418458\n",
      "train loss:0.597893417992277\n",
      "=== epoch:412, train acc:0.8566666666666667, test acc:0.6682 ===\n",
      "train loss:0.6379610580381597\n",
      "train loss:0.6039480736279748\n",
      "train loss:0.5869823646659021\n",
      "=== epoch:413, train acc:0.8566666666666667, test acc:0.6673 ===\n",
      "train loss:0.5155009336556811\n",
      "train loss:0.5771229311423219\n",
      "train loss:0.4511582410652334\n",
      "=== epoch:414, train acc:0.8533333333333334, test acc:0.6619 ===\n",
      "train loss:0.6048237568144406\n",
      "train loss:0.6599344294767248\n",
      "train loss:0.5462097799656084\n",
      "=== epoch:415, train acc:0.8566666666666667, test acc:0.666 ===\n",
      "train loss:0.6131161834049079\n",
      "train loss:0.5832518748224896\n",
      "train loss:0.6716215989049938\n",
      "=== epoch:416, train acc:0.8533333333333334, test acc:0.6645 ===\n",
      "train loss:0.5970488320873164\n",
      "train loss:0.6203545366745642\n",
      "train loss:0.6022588538702474\n",
      "=== epoch:417, train acc:0.8533333333333334, test acc:0.6624 ===\n",
      "train loss:0.4832850358267809\n",
      "train loss:0.6437910758555522\n",
      "train loss:0.580534023774229\n",
      "=== epoch:418, train acc:0.86, test acc:0.668 ===\n",
      "train loss:0.5061327873910575\n",
      "train loss:0.5470168590849888\n",
      "train loss:0.48932567223334766\n",
      "=== epoch:419, train acc:0.86, test acc:0.6673 ===\n",
      "train loss:0.630458067353821\n",
      "train loss:0.6058802783365337\n",
      "train loss:0.5807591597417868\n",
      "=== epoch:420, train acc:0.8633333333333333, test acc:0.6718 ===\n",
      "train loss:0.598604661233437\n",
      "train loss:0.5801475686220683\n",
      "train loss:0.5916013396400772\n",
      "=== epoch:421, train acc:0.86, test acc:0.6664 ===\n",
      "train loss:0.5543281387232285\n",
      "train loss:0.5521305849230159\n",
      "train loss:0.6140196474231913\n",
      "=== epoch:422, train acc:0.86, test acc:0.6751 ===\n",
      "train loss:0.49953275576139433\n",
      "train loss:0.5248581637508363\n",
      "train loss:0.5024081338373659\n",
      "=== epoch:423, train acc:0.8633333333333333, test acc:0.6803 ===\n",
      "train loss:0.5770424545759771\n",
      "train loss:0.5026676145880721\n",
      "train loss:0.4744515340267398\n",
      "=== epoch:424, train acc:0.8633333333333333, test acc:0.6795 ===\n",
      "train loss:0.5261498035129393\n",
      "train loss:0.5333411819389994\n",
      "train loss:0.45803140225777134\n",
      "=== epoch:425, train acc:0.8666666666666667, test acc:0.6833 ===\n",
      "train loss:0.6015984938053495\n",
      "train loss:0.4499872022958503\n",
      "train loss:0.5460277098479592\n",
      "=== epoch:426, train acc:0.8666666666666667, test acc:0.6848 ===\n",
      "train loss:0.4889221823085282\n",
      "train loss:0.5218816317267643\n",
      "train loss:0.48044623828223926\n",
      "=== epoch:427, train acc:0.8666666666666667, test acc:0.6818 ===\n",
      "train loss:0.43972286096653446\n",
      "train loss:0.5471563701547165\n",
      "train loss:0.511886587404732\n",
      "=== epoch:428, train acc:0.8633333333333333, test acc:0.6809 ===\n",
      "train loss:0.55971017467516\n",
      "train loss:0.593046200217881\n",
      "train loss:0.5479636249249378\n",
      "=== epoch:429, train acc:0.8666666666666667, test acc:0.6876 ===\n",
      "train loss:0.4522125047666174\n",
      "train loss:0.5528558099733359\n",
      "train loss:0.497029267692071\n",
      "=== epoch:430, train acc:0.8666666666666667, test acc:0.6861 ===\n",
      "train loss:0.387254757924\n",
      "train loss:0.5020935079945238\n",
      "train loss:0.5132816124891648\n",
      "=== epoch:431, train acc:0.8666666666666667, test acc:0.6872 ===\n",
      "train loss:0.4811811989322915\n",
      "train loss:0.5446742760870149\n",
      "train loss:0.5192001540165823\n",
      "=== epoch:432, train acc:0.8633333333333333, test acc:0.6851 ===\n",
      "train loss:0.6045610023041008\n",
      "train loss:0.492363041715677\n",
      "train loss:0.508884189236924\n",
      "=== epoch:433, train acc:0.8666666666666667, test acc:0.6874 ===\n",
      "train loss:0.5263217654027545\n",
      "train loss:0.459049293734037\n",
      "train loss:0.6009790327692809\n",
      "=== epoch:434, train acc:0.87, test acc:0.69 ===\n",
      "train loss:0.5593779180200217\n",
      "train loss:0.5126828029341134\n",
      "train loss:0.5110145203663096\n",
      "=== epoch:435, train acc:0.87, test acc:0.6891 ===\n",
      "train loss:0.4413817670051446\n",
      "train loss:0.519192997136698\n",
      "train loss:0.4615579791402873\n",
      "=== epoch:436, train acc:0.87, test acc:0.687 ===\n",
      "train loss:0.4394225451629651\n",
      "train loss:0.4725483180071282\n",
      "train loss:0.48800547416785234\n",
      "=== epoch:437, train acc:0.8666666666666667, test acc:0.6803 ===\n",
      "train loss:0.464892655831705\n",
      "train loss:0.46169879821375887\n",
      "train loss:0.3801980050995737\n",
      "=== epoch:438, train acc:0.87, test acc:0.6872 ===\n",
      "train loss:0.4636623741035658\n",
      "train loss:0.5423713783335081\n",
      "train loss:0.40442198582127736\n",
      "=== epoch:439, train acc:0.87, test acc:0.6885 ===\n",
      "train loss:0.5062502929059817\n",
      "train loss:0.5519809454579608\n",
      "train loss:0.5976202691447178\n",
      "=== epoch:440, train acc:0.8666666666666667, test acc:0.6877 ===\n",
      "train loss:0.5370342121620054\n",
      "train loss:0.5324322618954662\n",
      "train loss:0.4580888345224325\n",
      "=== epoch:441, train acc:0.87, test acc:0.689 ===\n",
      "train loss:0.4431312496357523\n",
      "train loss:0.42232784468700985\n",
      "train loss:0.49565846600797564\n",
      "=== epoch:442, train acc:0.87, test acc:0.6932 ===\n",
      "train loss:0.49909901876324336\n",
      "train loss:0.46068532283516334\n",
      "train loss:0.5552173125466904\n",
      "=== epoch:443, train acc:0.8666666666666667, test acc:0.6895 ===\n",
      "train loss:0.4281034568502321\n",
      "train loss:0.4230390449415873\n",
      "train loss:0.4695364001905502\n",
      "=== epoch:444, train acc:0.8666666666666667, test acc:0.6882 ===\n",
      "train loss:0.5486627087646165\n",
      "train loss:0.6002301056660924\n",
      "train loss:0.6152184960158074\n",
      "=== epoch:445, train acc:0.8666666666666667, test acc:0.6902 ===\n",
      "train loss:0.5451691724090325\n",
      "train loss:0.49796660644609125\n",
      "train loss:0.44588450513665984\n",
      "=== epoch:446, train acc:0.87, test acc:0.6905 ===\n",
      "train loss:0.3444119317548557\n",
      "train loss:0.5098710347120955\n",
      "train loss:0.44794183766999784\n",
      "=== epoch:447, train acc:0.87, test acc:0.6913 ===\n",
      "train loss:0.47299699224312747\n",
      "train loss:0.5701811713358712\n",
      "train loss:0.46635881555254843\n",
      "=== epoch:448, train acc:0.88, test acc:0.6921 ===\n",
      "train loss:0.47409723245588853\n",
      "train loss:0.5065746107756108\n",
      "train loss:0.44897133519740856\n",
      "=== epoch:449, train acc:0.8733333333333333, test acc:0.6945 ===\n",
      "train loss:0.3785757865739427\n",
      "train loss:0.4898072582543741\n",
      "train loss:0.4414599243785743\n",
      "=== epoch:450, train acc:0.8866666666666667, test acc:0.697 ===\n",
      "train loss:0.5463186792432927\n",
      "train loss:0.5261808373369676\n",
      "train loss:0.4468728720237705\n",
      "=== epoch:451, train acc:0.8833333333333333, test acc:0.6969 ===\n",
      "train loss:0.4906034646135142\n",
      "train loss:0.5284454311059621\n",
      "train loss:0.4565511950783167\n",
      "=== epoch:452, train acc:0.8833333333333333, test acc:0.6957 ===\n",
      "train loss:0.33448543239723344\n",
      "train loss:0.41732774672800105\n",
      "train loss:0.48020773036888786\n",
      "=== epoch:453, train acc:0.8733333333333333, test acc:0.695 ===\n",
      "train loss:0.4594374254633513\n",
      "train loss:0.4311916942843456\n",
      "train loss:0.3939640410338421\n",
      "=== epoch:454, train acc:0.8866666666666667, test acc:0.6973 ===\n",
      "train loss:0.41747332843573437\n",
      "train loss:0.5442110652855764\n",
      "train loss:0.3929906544143018\n",
      "=== epoch:455, train acc:0.8866666666666667, test acc:0.6976 ===\n",
      "train loss:0.41782771595056123\n",
      "train loss:0.5028951918514103\n",
      "train loss:0.501920221273017\n",
      "=== epoch:456, train acc:0.8866666666666667, test acc:0.6986 ===\n",
      "train loss:0.4032518824916136\n",
      "train loss:0.4276565299726874\n",
      "train loss:0.5444141708294462\n",
      "=== epoch:457, train acc:0.89, test acc:0.7012 ===\n",
      "train loss:0.46496408887548185\n",
      "train loss:0.4439296248084815\n",
      "train loss:0.420549262309989\n",
      "=== epoch:458, train acc:0.89, test acc:0.7005 ===\n",
      "train loss:0.4003825363744981\n",
      "train loss:0.4981601617102185\n",
      "train loss:0.41977129792741175\n",
      "=== epoch:459, train acc:0.8933333333333333, test acc:0.7013 ===\n",
      "train loss:0.4035171455034041\n",
      "train loss:0.38876299101954537\n",
      "train loss:0.4690056430304777\n",
      "=== epoch:460, train acc:0.89, test acc:0.6989 ===\n",
      "train loss:0.43870071873032346\n",
      "train loss:0.3974113856715667\n",
      "train loss:0.4317662218411097\n",
      "=== epoch:461, train acc:0.8866666666666667, test acc:0.6982 ===\n",
      "train loss:0.44309962105805717\n",
      "train loss:0.42922974762091753\n",
      "train loss:0.32698218575643123\n",
      "=== epoch:462, train acc:0.8866666666666667, test acc:0.6995 ===\n",
      "train loss:0.4310940715640496\n",
      "train loss:0.39396880156827563\n",
      "train loss:0.372202784291419\n",
      "=== epoch:463, train acc:0.8866666666666667, test acc:0.7007 ===\n",
      "train loss:0.43919073610760123\n",
      "train loss:0.39092686996636694\n",
      "train loss:0.5370659829692541\n",
      "=== epoch:464, train acc:0.89, test acc:0.6989 ===\n",
      "train loss:0.3799850870762259\n",
      "train loss:0.3674893187744149\n",
      "train loss:0.4643322090290781\n",
      "=== epoch:465, train acc:0.8866666666666667, test acc:0.701 ===\n",
      "train loss:0.4525666178966274\n",
      "train loss:0.40850607905666253\n",
      "train loss:0.4641847553025069\n",
      "=== epoch:466, train acc:0.89, test acc:0.6986 ===\n",
      "train loss:0.4273756144881827\n",
      "train loss:0.4444649068863333\n",
      "train loss:0.36697057062109467\n",
      "=== epoch:467, train acc:0.8866666666666667, test acc:0.6972 ===\n",
      "train loss:0.4407716945483633\n",
      "train loss:0.40358301440287775\n",
      "train loss:0.4592238224060256\n",
      "=== epoch:468, train acc:0.89, test acc:0.701 ===\n",
      "train loss:0.3780582593194724\n",
      "train loss:0.37327605695706345\n",
      "train loss:0.4208013909549363\n",
      "=== epoch:469, train acc:0.8966666666666666, test acc:0.7053 ===\n",
      "train loss:0.37025288943421075\n",
      "train loss:0.35942710597320804\n",
      "train loss:0.4244223056968156\n",
      "=== epoch:470, train acc:0.9, test acc:0.7055 ===\n",
      "train loss:0.4111164969363275\n",
      "train loss:0.4720437834277952\n",
      "train loss:0.4090951910528138\n",
      "=== epoch:471, train acc:0.9033333333333333, test acc:0.7048 ===\n",
      "train loss:0.38358216149046975\n",
      "train loss:0.3635661057968466\n",
      "train loss:0.40089836458939987\n",
      "=== epoch:472, train acc:0.9, test acc:0.7052 ===\n",
      "train loss:0.4812898480106994\n",
      "train loss:0.42145485085808176\n",
      "train loss:0.4617998824024806\n",
      "=== epoch:473, train acc:0.9, test acc:0.7066 ===\n",
      "train loss:0.31957151619065316\n",
      "train loss:0.4146859192586113\n",
      "train loss:0.4817228367233713\n",
      "=== epoch:474, train acc:0.9033333333333333, test acc:0.708 ===\n",
      "train loss:0.3340961762106335\n",
      "train loss:0.34678813329504005\n",
      "train loss:0.35628944903268644\n",
      "=== epoch:475, train acc:0.8966666666666666, test acc:0.7074 ===\n",
      "train loss:0.3633354904008908\n",
      "train loss:0.3217367492257881\n",
      "train loss:0.3263254747691258\n",
      "=== epoch:476, train acc:0.9033333333333333, test acc:0.708 ===\n",
      "train loss:0.4568348586082472\n",
      "train loss:0.3683642024733528\n",
      "train loss:0.4290811490778528\n",
      "=== epoch:477, train acc:0.9033333333333333, test acc:0.7098 ===\n",
      "train loss:0.3795491593465772\n",
      "train loss:0.47368513116779\n",
      "train loss:0.34819206089699634\n",
      "=== epoch:478, train acc:0.9066666666666666, test acc:0.7131 ===\n",
      "train loss:0.2963363284701907\n",
      "train loss:0.41014924617455534\n",
      "train loss:0.36177390046197144\n",
      "=== epoch:479, train acc:0.9033333333333333, test acc:0.7134 ===\n",
      "train loss:0.44972942580518027\n",
      "train loss:0.3667410603359936\n",
      "train loss:0.43604799162655733\n",
      "=== epoch:480, train acc:0.9033333333333333, test acc:0.713 ===\n",
      "train loss:0.432555897802926\n",
      "train loss:0.38771718237230507\n",
      "train loss:0.3026140002850524\n",
      "=== epoch:481, train acc:0.9066666666666666, test acc:0.7142 ===\n",
      "train loss:0.3785341339143929\n",
      "train loss:0.3651374914155346\n",
      "train loss:0.3483783690371041\n",
      "=== epoch:482, train acc:0.9033333333333333, test acc:0.7136 ===\n",
      "train loss:0.4322967264400972\n",
      "train loss:0.3758391389667871\n",
      "train loss:0.3063044627134617\n",
      "=== epoch:483, train acc:0.91, test acc:0.7112 ===\n",
      "train loss:0.2751946332153595\n",
      "train loss:0.28459134282186166\n",
      "train loss:0.4132686517422772\n",
      "=== epoch:484, train acc:0.9066666666666666, test acc:0.7122 ===\n",
      "train loss:0.41346128902715956\n",
      "train loss:0.3184612990338946\n",
      "train loss:0.3858068151358659\n",
      "=== epoch:485, train acc:0.9066666666666666, test acc:0.7125 ===\n",
      "train loss:0.42213882541449754\n",
      "train loss:0.29983720280952164\n",
      "train loss:0.3562865738480383\n",
      "=== epoch:486, train acc:0.9133333333333333, test acc:0.7132 ===\n",
      "train loss:0.3958541154511581\n",
      "train loss:0.33098843445252435\n",
      "train loss:0.29622777356988467\n",
      "=== epoch:487, train acc:0.91, test acc:0.7129 ===\n",
      "train loss:0.4421725820980598\n",
      "train loss:0.353609130459789\n",
      "train loss:0.43158013889437685\n",
      "=== epoch:488, train acc:0.91, test acc:0.7106 ===\n",
      "train loss:0.3127539207080158\n",
      "train loss:0.3711748950966819\n",
      "train loss:0.42474409976972266\n",
      "=== epoch:489, train acc:0.91, test acc:0.7128 ===\n",
      "train loss:0.44038464577069925\n",
      "train loss:0.3542146001047275\n",
      "train loss:0.3896806185615616\n",
      "=== epoch:490, train acc:0.9066666666666666, test acc:0.7113 ===\n",
      "train loss:0.38502373532098033\n",
      "train loss:0.2976783318674021\n",
      "train loss:0.41863768979831784\n",
      "=== epoch:491, train acc:0.9066666666666666, test acc:0.7084 ===\n",
      "train loss:0.3416001133864339\n",
      "train loss:0.36536893608098475\n",
      "train loss:0.32282884526184086\n",
      "=== epoch:492, train acc:0.9133333333333333, test acc:0.7117 ===\n",
      "train loss:0.42056614647323753\n",
      "train loss:0.4020596322604809\n",
      "train loss:0.2965787203037715\n",
      "=== epoch:493, train acc:0.91, test acc:0.7123 ===\n",
      "train loss:0.3335867360158091\n",
      "train loss:0.40736035803153475\n",
      "train loss:0.3890158442473128\n",
      "=== epoch:494, train acc:0.91, test acc:0.713 ===\n",
      "train loss:0.3820055233322326\n",
      "train loss:0.33114291203057616\n",
      "train loss:0.4286751828335298\n",
      "=== epoch:495, train acc:0.91, test acc:0.7112 ===\n",
      "train loss:0.324563968923688\n",
      "train loss:0.3600029533111083\n",
      "train loss:0.3380573813618352\n",
      "=== epoch:496, train acc:0.91, test acc:0.7117 ===\n",
      "train loss:0.42546542249958125\n",
      "train loss:0.3750524211316955\n",
      "train loss:0.4008979907015504\n",
      "=== epoch:497, train acc:0.9133333333333333, test acc:0.7129 ===\n",
      "train loss:0.3667057638349188\n",
      "train loss:0.3352661927588981\n",
      "train loss:0.36758424649086036\n",
      "=== epoch:498, train acc:0.9133333333333333, test acc:0.7139 ===\n",
      "train loss:0.3303543292078118\n",
      "train loss:0.3589349156768912\n",
      "train loss:0.33613294250678344\n",
      "=== epoch:499, train acc:0.9133333333333333, test acc:0.716 ===\n",
      "train loss:0.3502912594181477\n",
      "train loss:0.3431021832819758\n",
      "train loss:0.31512094513402583\n",
      "=== epoch:500, train acc:0.9133333333333333, test acc:0.715 ===\n",
      "train loss:0.2761862442523443\n",
      "train loss:0.2893425563875567\n",
      "train loss:0.3207680247813709\n",
      "=== epoch:501, train acc:0.9133333333333333, test acc:0.7171 ===\n",
      "train loss:0.27642417562607857\n",
      "train loss:0.25267675621100766\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7161\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T10:59:00.064633Z",
     "start_time": "2024-10-19T10:58:59.992856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, color='k', marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, color='r', marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.03)\n",
    "plt.legend(loc='upper left')\n",
    "# plt.legend(loc='lower right')\n",
    "box_off()"
   ],
   "id": "12b9d50d01f5a5e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
